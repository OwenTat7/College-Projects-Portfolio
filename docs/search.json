[
  {
    "objectID": "assets/drive-download-20250925T232328Z-1-001/1_EDA_Cleaning.html",
    "href": "assets/drive-download-20250925T232328Z-1-001/1_EDA_Cleaning.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\ndf.info()\ndf.head()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-1-6da2b58110ee&gt; in &lt;cell line: 0&gt;()\n      1 import pandas as pd\n----&gt; 2 df = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\n      3 df.info()\n      4 df.head()\n\n/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1024     kwds.update(kwds_defaults)\n   1025 \n-&gt; 1026     return _read(filepath_or_buffer, kwds)\n   1027 \n   1028 \n\n/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py in _read(filepath_or_buffer, kwds)\n    618 \n    619     # Create the parser.\n--&gt; 620     parser = TextFileReader(filepath_or_buffer, **kwds)\n    621 \n    622     if chunksize or iterator:\n\n/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py in __init__(self, f, engine, **kwds)\n   1618 \n   1619         self.handles: IOHandles | None = None\n-&gt; 1620         self._engine = self._make_engine(f, self.engine)\n   1621 \n   1622     def close(self) -&gt; None:\n\n/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py in _make_engine(self, f, engine)\n   1878                 if \"b\" not in mode:\n   1879                     mode += \"b\"\n-&gt; 1880             self.handles = get_handle(\n   1881                 f,\n   1882                 mode,\n\n/usr/local/lib/python3.11/dist-packages/pandas/io/common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    871         if ioargs.encoding and \"b\" not in ioargs.mode:\n    872             # Encoding\n--&gt; 873             handle = open(\n    874                 handle,\n    875                 ioargs.mode,\n\nFileNotFoundError: [Errno 2] No such file or directory: 'heart_2020_cleaned.csv'\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n# Plotting the number of people with and without heart disease\nplt.figure(figsize=(8, 6))\ndf['HeartDisease'].value_counts().plot(kind='bar')\nplt.title('Number of People With and Without Heart Disease (First 50,000 Rows)')\nplt.xlabel('Heart Disease')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nBMI\nPhysicalHealth\nMentalHealth\nSleepTime\n\n\n\n\ncount\n50000.000000\n50000.000000\n50000.000000\n50000.00000\n\n\nmean\n27.971388\n3.539560\n3.984260\n7.12938\n\n\nstd\n6.239799\n8.094921\n7.979439\n1.49613\n\n\nmin\n12.400000\n0.000000\n0.000000\n1.00000\n\n\n25%\n23.710000\n0.000000\n0.000000\n6.00000\n\n\n50%\n26.960000\n0.000000\n0.000000\n7.00000\n\n\n75%\n31.010000\n2.000000\n4.000000\n8.00000\n\n\nmax\n87.050000\n30.000000\n30.000000\n24.00000\n\n\n\n\n\n\n\n\nbinary_cols = [\n    'HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke',\n    'DiffWalking', 'Diabetic', 'PhysicalActivity',\n    'Asthma', 'KidneyDisease', 'SkinCancer'\n]\n\ndf[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\nC:\\Users\\modyx\\AppData\\Local\\Temp\\ipykernel_21892\\1360497000.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\nRace\nDiabetic\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\nFemale\n55-59\nWhite\n1\n1\nVery good\n5.0\n1\n0\n1\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\nFemale\n80 or older\nWhite\n0\n1\nVery good\n7.0\n0\n0\n0\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\nMale\n65-69\nWhite\n1\n1\nFair\n8.0\n1\n0\n0\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\nFemale\n75-79\nWhite\n0\n0\nGood\n6.0\n0\n0\n1\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\nFemale\n40-44\nWhite\n0\n1\nVery good\n8.0\n0\n0\n0\n\n\n\n\n\n\n\n\n## Male is  1, Female is 0\ndf['Sex'] = df['Sex'].map({'Male': 1, 'Female': 0})\n\n\ndf['ComorbidityCount'] = df[['Stroke', 'Diabetic', 'Asthma', 'KidneyDisease', 'SkinCancer']].sum(axis=1)\n\n\ndf['UnhealthyDays'] = df['PhysicalHealth'] + df['MentalHealth']\ndf['UnhealthyDays'] = df['UnhealthyDays'].clip(upper=30)\n\n\ndf['RiskBehavior'] = ((df['Smoking'] == 1) | (df['AlcoholDrinking'] == 1)).astype(int)\n\n\ndf['SleepCategory'] = pd.cut(df['SleepTime'],\n                             bins=[0, 5, 6.9, 8.9, 24],\n                             labels=['Very Short', 'Short', 'Normal', 'Long'])\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\n...\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\nComorbidityCount\nUnhealthyDays\nRiskBehavior\nSleepCategory\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n55-59\n...\n1\nVery good\n5.0\n1\n0\n1\n3\n30.0\n1\nVery Short\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n80 or older\n...\n1\nVery good\n7.0\n0\n0\n0\n1\n0.0\n0\nNormal\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n65-69\n...\n1\nFair\n8.0\n1\n0\n0\n2\n30.0\n1\nNormal\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n75-79\n...\n0\nGood\n6.0\n0\n0\n1\n1\n0.0\n0\nShort\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n40-44\n...\n1\nVery good\n8.0\n0\n0\n0\n0\n28.0\n0\nNormal\n\n\n\n\n5 rows × 22 columns\n\n\n\n\ndf.columns\n\nIndex(['HeartDisease', 'BMI', 'Smoking', 'AlcoholDrinking', 'Stroke',\n       'PhysicalHealth', 'MentalHealth', 'DiffWalking', 'Sex', 'AgeCategory',\n       'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth', 'SleepTime',\n       'Asthma', 'KidneyDisease', 'SkinCancer', 'ComorbidityCount',\n       'UnhealthyDays', 'RiskBehavior', 'SleepCategory'],\n      dtype='object')\n\n\n\ndf.to_csv(\"heart_cleaned_final.csv\", index=False)\n\n\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n\nX = df.drop(\"HeartDisease\", axis=1)\ny = df[\"HeartDisease\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Step 3: Save each set as a Pickle (.p) file\npickle.dump(X_train, open(\"X_train.p\", \"wb\"))\npickle.dump(X_test, open(\"X_test.p\", \"wb\"))\npickle.dump(y_train, open(\"y_train.p\", \"wb\"))\npickle.dump(y_test, open(\"y_test.p\", \"wb\"))\n\nprint(\"Pickle files saved successfully!\")\n\nPickle files saved successfully!"
  },
  {
    "objectID": "assets/drive-download-20250925T232328Z-1-001/4_Random Forests.html",
    "href": "assets/drive-download-20250925T232328Z-1-001/4_Random Forests.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\ndf.info()\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 18 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   HeartDisease      50000 non-null  object \n 1   BMI               50000 non-null  float64\n 2   Smoking           50000 non-null  object \n 3   AlcoholDrinking   50000 non-null  object \n 4   Stroke            50000 non-null  object \n 5   PhysicalHealth    50000 non-null  float64\n 6   MentalHealth      50000 non-null  float64\n 7   DiffWalking       50000 non-null  object \n 8   Sex               50000 non-null  object \n 9   AgeCategory       50000 non-null  object \n 10  Race              50000 non-null  object \n 11  Diabetic          50000 non-null  object \n 12  PhysicalActivity  50000 non-null  object \n 13  GenHealth         50000 non-null  object \n 14  SleepTime         50000 non-null  float64\n 15  Asthma            50000 non-null  object \n 16  KidneyDisease     50000 non-null  object \n 17  SkinCancer        50000 non-null  object \ndtypes: float64(4), object(14)\nmemory usage: 6.9+ MB\n\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\nRace\nDiabetic\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\n\n\n\n\n0\nNo\n16.60\nYes\nNo\nNo\n3.0\n30.0\nNo\nFemale\n55-59\nWhite\nYes\nYes\nVery good\n5.0\nYes\nNo\nYes\n\n\n1\nNo\n20.34\nNo\nNo\nYes\n0.0\n0.0\nNo\nFemale\n80 or older\nWhite\nNo\nYes\nVery good\n7.0\nNo\nNo\nNo\n\n\n2\nNo\n26.58\nYes\nNo\nNo\n20.0\n30.0\nNo\nMale\n65-69\nWhite\nYes\nYes\nFair\n8.0\nYes\nNo\nNo\n\n\n3\nNo\n24.21\nNo\nNo\nNo\n0.0\n0.0\nNo\nFemale\n75-79\nWhite\nNo\nNo\nGood\n6.0\nNo\nNo\nYes\n\n\n4\nNo\n23.71\nNo\nNo\nNo\n28.0\n0.0\nYes\nFemale\n40-44\nWhite\nNo\nYes\nVery good\n8.0\nNo\nNo\nNo\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n# Plotting the number of people with and without heart disease\nplt.figure(figsize=(8, 6))\ndf['HeartDisease'].value_counts().plot(kind='bar')\nplt.title('Number of People With and Without Heart Disease (First 50,000 Rows)')\nplt.xlabel('Heart Disease')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nBMI\nPhysicalHealth\nMentalHealth\nSleepTime\n\n\n\n\ncount\n50000.000000\n50000.000000\n50000.000000\n50000.00000\n\n\nmean\n27.971388\n3.539560\n3.984260\n7.12938\n\n\nstd\n6.239799\n8.094921\n7.979439\n1.49613\n\n\nmin\n12.400000\n0.000000\n0.000000\n1.00000\n\n\n25%\n23.710000\n0.000000\n0.000000\n6.00000\n\n\n50%\n26.960000\n0.000000\n0.000000\n7.00000\n\n\n75%\n31.010000\n2.000000\n4.000000\n8.00000\n\n\nmax\n87.050000\n30.000000\n30.000000\n24.00000\n\n\n\n\n\n\n\n\nbinary_cols = [\n    'HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke',\n    'DiffWalking', 'Diabetic', 'PhysicalActivity',\n    'Asthma', 'KidneyDisease', 'SkinCancer'\n]\n\ndf[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\nC:\\Users\\modyx\\AppData\\Local\\Temp\\ipykernel_21892\\1360497000.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\nRace\nDiabetic\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\nFemale\n55-59\nWhite\n1\n1\nVery good\n5.0\n1\n0\n1\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\nFemale\n80 or older\nWhite\n0\n1\nVery good\n7.0\n0\n0\n0\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\nMale\n65-69\nWhite\n1\n1\nFair\n8.0\n1\n0\n0\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\nFemale\n75-79\nWhite\n0\n0\nGood\n6.0\n0\n0\n1\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\nFemale\n40-44\nWhite\n0\n1\nVery good\n8.0\n0\n0\n0\n\n\n\n\n\n\n\n\n## Male is  1, Female is 0\ndf['Sex'] = df['Sex'].map({'Male': 1, 'Female': 0})\n\n\ndf['ComorbidityCount'] = df[['Stroke', 'Diabetic', 'Asthma', 'KidneyDisease', 'SkinCancer']].sum(axis=1)\n\n\ndf['UnhealthyDays'] = df['PhysicalHealth'] + df['MentalHealth']\ndf['UnhealthyDays'] = df['UnhealthyDays'].clip(upper=30)\n\n\ndf['RiskBehavior'] = ((df['Smoking'] == 1) | (df['AlcoholDrinking'] == 1)).astype(int)\n\n\ndf['SleepCategory'] = pd.cut(df['SleepTime'],\n                             bins=[0, 5, 6.9, 8.9, 24],\n                             labels=['Very Short', 'Short', 'Normal', 'Long'])\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\n...\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\nComorbidityCount\nUnhealthyDays\nRiskBehavior\nSleepCategory\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n55-59\n...\n1\nVery good\n5.0\n1\n0\n1\n3\n30.0\n1\nVery Short\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n80 or older\n...\n1\nVery good\n7.0\n0\n0\n0\n1\n0.0\n0\nNormal\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n65-69\n...\n1\nFair\n8.0\n1\n0\n0\n2\n30.0\n1\nNormal\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n75-79\n...\n0\nGood\n6.0\n0\n0\n1\n1\n0.0\n0\nShort\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n40-44\n...\n1\nVery good\n8.0\n0\n0\n0\n0\n28.0\n0\nNormal\n\n\n\n\n5 rows × 22 columns\n\n\n\n\ndf.columns\n\nIndex(['HeartDisease', 'BMI', 'Smoking', 'AlcoholDrinking', 'Stroke',\n       'PhysicalHealth', 'MentalHealth', 'DiffWalking', 'Sex', 'AgeCategory',\n       'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth', 'SleepTime',\n       'Asthma', 'KidneyDisease', 'SkinCancer', 'ComorbidityCount',\n       'UnhealthyDays', 'RiskBehavior', 'SleepCategory'],\n      dtype='object')\n\n\n\ndf.to_csv(\"heart_cleaned_final.csv\", index=False)\n\n\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n\nX = df.drop(\"HeartDisease\", axis=1)\ny = df[\"HeartDisease\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Step 3: Save each set as a Pickle (.p) file\npickle.dump(X_train, open(\"X_train.p\", \"wb\"))\npickle.dump(X_test, open(\"X_test.p\", \"wb\"))\npickle.dump(y_train, open(\"y_train.p\", \"wb\"))\npickle.dump(y_test, open(\"y_test.p\", \"wb\"))\n\nprint(\"Pickle files saved successfully!\")\n\nPickle files saved successfully!\n\n\n\n# 1. Imports\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport matplotlib.pyplot as plt\n\n# 2. Load and preprocess the data\ndf = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\n\n# Convert binary columns\nbinary_cols = [\n    'HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke',\n    'DiffWalking', 'Diabetic', 'PhysicalActivity',\n    'Asthma', 'KidneyDisease', 'SkinCancer'\n]\ndf[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n# Convert Sex\ndf['Sex'] = df['Sex'].map({'Male': 1, 'Female': 0})\n\n# Feature engineering\ndf['ComorbidityCount'] = df[['Stroke', 'Diabetic', 'Asthma', 'KidneyDisease', 'SkinCancer']].sum(axis=1)\ndf['UnhealthyDays'] = (df['PhysicalHealth'] + df['MentalHealth']).clip(upper=30)\ndf['RiskBehavior'] = ((df['Smoking'] == 1) | (df['AlcoholDrinking'] == 1)).astype(int)\ndf['SleepCategory'] = pd.cut(df['SleepTime'],\n                             bins=[0, 5, 6.9, 8.9, 24],\n                             labels=['Very Short', 'Short', 'Normal', 'Long'])\n\n# One-hot encode all categorical (object) features\ndf = pd.get_dummies(df)\n\n# Drop rows with missing values\ndf = df.dropna()\n\n# 3. Train-test split\nX = df.drop(\"HeartDisease\", axis=1)\ny = df[\"HeartDisease\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# 4. Train Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# 5. Evaluate model\ny_pred = rf_model.predict(X_test)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\nprint(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n\n# 6. Feature Importance Plot\nimportances = rf_model.feature_importances_\nfeatures = X_train.columns\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(12, 6))\nplt.title(\"Feature Importances (Random Forest)\")\nplt.bar(range(len(importances)), importances[indices])\nplt.xticks(range(len(importances)), features[indices], rotation=90)\nplt.tight_layout()\nplt.grid(axis='y')\nplt.show()\n\n# 7. Save model and data splits\npickle.dump(rf_model, open(\"random_forest_model.p\", \"wb\"))\npickle.dump(X_train, open(\"X_train.p\", \"wb\"))\npickle.dump(X_test, open(\"X_test.p\", \"wb\"))\npickle.dump(y_train, open(\"y_train.p\", \"wb\"))\npickle.dump(y_test, open(\"y_test.p\", \"wb\"))\n\n\n\n/var/folders/66/l7dnx1g128b7j804pfwmzc480000gn/T/ipykernel_32451/4278937313.py:19: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\nConfusion Matrix:\n[[13455   239]\n [ 1164   142]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.92      0.98      0.95     13694\n           1       0.37      0.11      0.17      1306\n\n    accuracy                           0.91     15000\n   macro avg       0.65      0.55      0.56     15000\nweighted avg       0.87      0.91      0.88     15000\n\n\nAccuracy Score: 0.9064666666666666"
  },
  {
    "objectID": "assets/drive-download-20250925T232328Z-1-001/3_Ridge Regression.html",
    "href": "assets/drive-download-20250925T232328Z-1-001/3_Ridge Regression.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load dataset\ndf = pd.read_csv(r\"C:\\Users\\Michael Antonucci\\OneDrive\\Documents\\MISY331 Final Project\\heart_cleaned_final.csv\")\n\n# Convert categorical columns to numeric\ndf_encoded = pd.get_dummies(df, drop_first=True)\n\n# Define features and target\nX = df_encoded.drop(\"HeartDisease\", axis=1)\ny = df_encoded[\"HeartDisease\"]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# Normalize features\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Range of alpha values\nalphas = np.linspace(1, 10000, 300)\n\ncoefs = []\n\n# Fit Ridge regression for each alpha\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_train_scaled, y_train)\n    coefs.append(ridge.coef_)\n\ncoefs = np.array(coefs)\n\n# Evaluate Ridge model at alpha ~ 8000\nalpha_index = np.abs(alphas - 8000).argmin()\nridge_final = Ridge(alpha=alphas[alpha_index])\nridge_final.fit(X_train_scaled, y_train)\n\n# Predictions and performance\ny_pred = ridge_final.predict(X_test_scaled)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Ridge Regression at alpha={alphas[alpha_index]:.2f}\")\nprint(f\"Mean Squared Error: {mse:.4f}\")\nprint(f\"R-squared: {r2:.4f}\")\n\n# Top 5 most influential features at alpha ~8000\ntop_5_indices = np.argsort(np.abs(coefs[alpha_index]))[-5:]\ntop_5_features = np.array(X.columns)[top_5_indices]\nprint(\"Top 5 important features at alpha ≈ 8000:\", top_5_features)\n\n# Spaghetti plot for top 5 features\nplt.figure(figsize=(10, 6))\nfor i in top_5_indices:\n    plt.plot(alphas, coefs[:, i], label=X.columns[i])\n\nplt.xscale(\"log\")\nplt.xlabel(\"Alpha (log scale)\")\nplt.ylabel(\"Coefficient Value\")\nplt.title(\"Ridge Coefficient Paths for Top 5 Features\")\nplt.legend()\n\nRidge Regression at alpha=7993.51\nMean Squared Error: 0.0680\nR-squared: 0.1129\nTop 5 important features at alpha ≈ 8000: ['GenHealth_Poor' 'Age_80 or older' 'DiffWalking' 'Stroke' 'Diabetic']\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n\n# Prepare your data\nX = df_encoded.drop(\"HeartDisease\", axis=1)\ny = df_encoded[\"HeartDisease\"]\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit Ridge Regression model\nridge = Ridge(alpha=1.0)  # You can tune alpha\nridge.fit(X_train, y_train)\n\n# Predict on test set\ny_pred_continuous = ridge.predict(X_test)\n\n# Convert continuous output to binary labels (threshold at 0.5)\ny_pred_binary = (y_pred_continuous &gt;= 0.5).astype(int)\n\n# Evaluate metrics\naccuracy = accuracy_score(y_test, y_pred_binary)\nprecision = precision_score(y_test, y_pred_binary)\nrecall = recall_score(y_test, y_pred_binary)\ncm = confusion_matrix(y_test, y_pred_binary)\n\n# Print results\nprint(\"Ridge Regression Evaluation Metrics:\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\nRidge Regression Evaluation Metrics:\nAccuracy : 0.9189\nPrecision: 0.6154\nRecall   : 0.0484\n\nConfusion Matrix:\n[[9149   25]\n [ 786   40]]"
  },
  {
    "objectID": "assets/drive-download-20250925T232328Z-1-001/6_Bayesian Network.html",
    "href": "assets/drive-download-20250925T232328Z-1-001/6_Bayesian Network.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pyAgrum.skbn as skbn\nimport pyAgrum.lib.notebook as gnb\n\nfrom sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, RocCurveDisplay, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, RandomizedSearchCV\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport sample_data as sd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\nfrom sklearn.inspection import permutation_importance\n\nfrom IPython.display import display,HTML\ndf = pd.read_csv(\"/Users/NathanSwan 1/Downloads/heart_cleaned_final2.csv\", nrows=50000)\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nDiabetic\n...\nRace_White\nGenHealth_Excellent\nGenHealth_Fair\nGenHealth_Good\nGenHealth_Poor\nGenHealth_Very good\nSleepCategory_Very Short\nSleepCategory_Short\nSleepCategory_Normal\nSleepCategory_Long\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n1\n...\n1\n0\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n1\n...\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n0\n...\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n5 rows × 46 columns\n\n\n\n\nimport pickle\n\n# adjust paths/filenames as needed\nwith open('/Users/NathanSwan 1/Downloads/331 Project/X_train.p', 'rb') as f:\n    X_train = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/X_test.p', 'rb') as f:\n    X_test = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/y_train.p', 'rb') as f:\n    y_train = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/y_test.p', 'rb') as f:\n    y_test = pickle.load(f)\n\n# quick check\nprint(X_train.shape, X_test.shape, len(y_train), len(y_test))\n\n(35000, 21) (15000, 21) 35000 15000\n\n\nTree Augemented Network Bayesian Network\n\n\n\nbnc = skbn.BNClassifier(learningMethod = 'TAN') ## Tree-Augmented Network.\n\nbnc.fit(X_train, y_train)\n\ny_train_hat = bnc.predict(X_train)\ny_test_hat = bnc.predict(X_test)\n\nbnc.score(X_train, y_train), bnc.score(X_test, y_test)\n\n/opt/anaconda3/envs/MISY331__base/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/MISY331__base/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/MISY331__base/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n\n\n(0.7485142857142857, 0.7406)\n\n\n\ngnb.showBN(bnc.bn)\n\n\n\n\n\n\n\n\n\nwith open('bayesian_network_model.pkl', 'wb') as file:\n    pickle.dump(bnc.bn, file)"
  },
  {
    "objectID": "assets/drive-download-20250925T232328Z-1-001/2_Pre-Analysis Visualization.html",
    "href": "assets/drive-download-20250925T232328Z-1-001/2_Pre-Analysis Visualization.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"/Users/NathanSwan 1/Downloads/heart_cleaned_final2.csv\", nrows=50000)\ndf.info()\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 46 columns):\n #   Column                               Non-Null Count  Dtype  \n---  ------                               --------------  -----  \n 0   HeartDisease                         50000 non-null  int64  \n 1   BMI                                  50000 non-null  float64\n 2   Smoking                              50000 non-null  int64  \n 3   AlcoholDrinking                      50000 non-null  int64  \n 4   Stroke                               50000 non-null  int64  \n 5   PhysicalHealth                       50000 non-null  float64\n 6   MentalHealth                         50000 non-null  float64\n 7   DiffWalking                          50000 non-null  int64  \n 8   Sex                                  50000 non-null  int64  \n 9   Diabetic                             50000 non-null  int64  \n 10  PhysicalActivity                     50000 non-null  int64  \n 11  SleepTime                            50000 non-null  float64\n 12  Asthma                               50000 non-null  int64  \n 13  KidneyDisease                        50000 non-null  int64  \n 14  SkinCancer                           50000 non-null  int64  \n 15  ComorbidityCount                     50000 non-null  int64  \n 16  UnhealthyDays                        50000 non-null  float64\n 17  RiskBehavior                         50000 non-null  int64  \n 18  Age_18-24                            50000 non-null  int64  \n 19  Age_25-29                            50000 non-null  int64  \n 20  Age_30-34                            50000 non-null  int64  \n 21  Age_35-39                            50000 non-null  int64  \n 22  Age_40-44                            50000 non-null  int64  \n 23  Age_45-49                            50000 non-null  int64  \n 24  Age_50-54                            50000 non-null  int64  \n 25  Age_55-59                            50000 non-null  int64  \n 26  Age_60-64                            50000 non-null  int64  \n 27  Age_65-69                            50000 non-null  int64  \n 28  Age_70-74                            50000 non-null  int64  \n 29  Age_75-79                            50000 non-null  int64  \n 30  Age_80 or older                      50000 non-null  int64  \n 31  Race_American Indian/Alaskan Native  50000 non-null  int64  \n 32  Race_Asian                           50000 non-null  int64  \n 33  Race_Black                           50000 non-null  int64  \n 34  Race_Hispanic                        50000 non-null  int64  \n 35  Race_Other                           50000 non-null  int64  \n 36  Race_White                           50000 non-null  int64  \n 37  GenHealth_Excellent                  50000 non-null  int64  \n 38  GenHealth_Fair                       50000 non-null  int64  \n 39  GenHealth_Good                       50000 non-null  int64  \n 40  GenHealth_Poor                       50000 non-null  int64  \n 41  GenHealth_Very good                  50000 non-null  int64  \n 42  SleepCategory_Very Short             50000 non-null  int64  \n 43  SleepCategory_Short                  50000 non-null  int64  \n 44  SleepCategory_Normal                 50000 non-null  int64  \n 45  SleepCategory_Long                   50000 non-null  int64  \ndtypes: float64(5), int64(41)\nmemory usage: 17.5 MB\n\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nDiabetic\n...\nRace_White\nGenHealth_Excellent\nGenHealth_Fair\nGenHealth_Good\nGenHealth_Poor\nGenHealth_Very good\nSleepCategory_Very Short\nSleepCategory_Short\nSleepCategory_Normal\nSleepCategory_Long\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n1\n...\n1\n0\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n1\n...\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n0\n...\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n5 rows × 46 columns\n\n\n\n\nDistribution of the Dependent Variable. 0 meaning they do not have heart disease and 1 meaning they do have heart disease\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nhd_counts = df['HeartDisease'].value_counts().sort_index()\nplt.figure(figsize=(6,4))\nplt.bar(hd_counts.index, hd_counts.values)\nplt.xticks([0,1])\nplt.xlabel('Heart Disease Presence')\nplt.ylabel('Count')\nplt.title('Distribution of Heart Disease')\nplt.show()\n\n\n\n\n\n\n\n\n\nSmoking Status. 0 meaning that the patient does not smoke and 1 meaning that they do smoke.\n\n\nsmoke_counts = df['Smoking'].value_counts()\nplt.figure(figsize=(6,4))\nplt.bar(smoke_counts.index, smoke_counts.values)\nplt.xticks([0,1])\nplt.xlabel('Smoking Status')\nplt.ylabel('Count')\nplt.title('Distribution of Smoking Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nBMI Distribution By Heart Disease Status\n\n\nplt.figure(figsize=(8,5))\nsns.histplot(data=df, x=\"BMI\", hue=\"HeartDisease\", kde=True, bins=40, palette=\"coolwarm\", element=\"step\")\nplt.title(\"BMI Distribution by Heart Disease Status\")\nplt.xlabel(\"BMI\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAlcohol Drinking Status. 0 means they do not drink and 1 means they do drink.\n\n\nalc_counts = df['AlcoholDrinking'].value_counts()\nplt.figure(figsize=(6,4))\nplt.bar(alc_counts.index, alc_counts.values)\nplt.xticks([0,1])\nplt.xlabel('Alcohol Drinking Status')\nplt.ylabel('Count')\nplt.title('Distribution of Alcohol Drinking')\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Data-driven student showcasing projects in analytics, finance, modeling, and communication. Showcasing not just my projects throughout the years, but also my growth and development as a student.\n\nView Projects GitHub LinkedIn"
  },
  {
    "objectID": "index.html#creative-projects-freshman-year",
    "href": "index.html#creative-projects-freshman-year",
    "title": "Welcome",
    "section": "Creative Projects — Freshman Year",
    "text": "Creative Projects — Freshman Year\n\n\n\n\n\n\n\n\nEssays\n\nGenerational Influence of Technology Research Essay.docx\n\n\nDocument Preview: Click the link above to download and view the essay.\nNote: For best viewing experience, download the document to open in Microsoft Word or Google Docs.\n\n\nEnglish Research Project.docx\n\n\nDocument Preview: Click the link above to download and view the essay.\nNote: For best viewing experience, download the document to open in Microsoft Word or Google Docs."
  },
  {
    "objectID": "index.html#case-1-liz-motors-junior-year",
    "href": "index.html#case-1-liz-motors-junior-year",
    "title": "Welcome",
    "section": "Case 1 — Liz Motors — Junior Year",
    "text": "Case 1 — Liz Motors — Junior Year\nGroup project completed in FINC312 Intermediate Financial Management at the University of Delaware\n\n\n\nPDF: Case 1 - Liz Motors.pdf\nPDF: Case 1 Liz Motors.pdf"
  },
  {
    "objectID": "index.html#case-ii-mm-pizza-junior-year",
    "href": "index.html#case-ii-mm-pizza-junior-year",
    "title": "Welcome",
    "section": "Case II — M&M Pizza — Junior Year",
    "text": "Case II — M&M Pizza — Junior Year\nGroup project completed in FINC312 Intermediate Financial Management at the University of Delaware\n\n\n\nPDF: Case II M&M Pizza.pdf\nPDF: M&M Pizza copy.pdf"
  },
  {
    "objectID": "index.html#scoring-model-ahp-model-project-junior-year",
    "href": "index.html#scoring-model-ahp-model-project-junior-year",
    "title": "Welcome",
    "section": "Scoring Model / AHP Model Project — Junior Year",
    "text": "Scoring Model / AHP Model Project — Junior Year\n\n\n\n\nYour browser does not support the video tag. \n:::\n\n\nDownload video (720p MP4)\n\n\n:::\n\nMISY 331: Machine Learning for Business Final Project\nGroup project completed in MISY331 Machine Learning for Business at the University of Delaware\n\n\n\nPDF: MISY331 Final.pdf\n\n\n\n\n\nProject Notebooks (HTML Format):\n\nData Preprocessing: - 1. EDA & Cleaning - 2. Pre-Analysis Visualization\nMachine Learning Models: - 3. Ridge Regression - 4. Random Forests - 5. Boosted Tree - 6. Bayesian Network - 7. Clustering - 8. Neural Network\nResults & Visualization: - 9. Result Visualization - Model File: xgb_model2.pkl\n\nThis project demonstrates comprehensive machine learning techniques including data preprocessing, multiple modeling approaches, and result presentation and visualization.\n\n\n\n\nResume\n\n\n\nPDF: Owen Tatlonghari Resume.pdf\n\n\n\n\n\n\n\n\n\nContact\n\nGitHub: https://github.com/OwenTat7\nLinkedIn: https://linkedin.com/in/owen-tatlonghari"
  },
  {
    "objectID": "index.html#misy-331-machine-learning-for-business-final-project",
    "href": "index.html#misy-331-machine-learning-for-business-final-project",
    "title": "Welcome",
    "section": "MISY 331: Machine Learning for Business Final Project",
    "text": "MISY 331: Machine Learning for Business Final Project\nGroup project completed in MISY331 Machine Learning for Business at the University of Delaware\n\n\n\nPDF: MISY331 Final.pdf\n\n\n\n\n\nProject Notebooks (HTML Format):\n\nData Preprocessing: - 1. EDA & Cleaning - 2. Pre-Analysis Visualization\nMachine Learning Models: - 3. Ridge Regression - 4. Random Forests - 5. Boosted Tree - 6. Bayesian Network - 7. Clustering - 8. Neural Network\nResults & Visualization: - 9. Result Visualization - Model File: xgb_model2.pkl\n\nThis project demonstrates comprehensive machine learning techniques including data preprocessing, multiple modeling approaches, and result presentation and visualization."
  },
  {
    "objectID": "index.html#resume",
    "href": "index.html#resume",
    "title": "Welcome",
    "section": "Resume",
    "text": "Resume\n\n\n\nPDF: Owen Tatlonghari Resume.pdf"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Welcome",
    "section": "Contact",
    "text": "Contact\n\nGitHub: https://github.com/OwenTat7\nLinkedIn: https://linkedin.com/in/owen-tatlonghari"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Owen. This site collects my projects, coursework, and media.\n\nEmail: octat@udel.edu\nPhone: 267-481-3233\nResume: Owen Tatlonghari Resume.pdf\n\n\n\n\n\nThis website is built with Quarto and deployed via GitHub Pages."
  },
  {
    "objectID": "assets/drive-download-20250925T232328Z-1-001/7_Clustering.html",
    "href": "assets/drive-download-20250925T232328Z-1-001/7_Clustering.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\n\n# 1. Load cleaned dataset\ndf = pd.read_csv(r\"C:\\Users\\Michael Antonucci\\OneDrive\\Documents\\MISY331 Final Project\\heart_cleaned_final.csv\")\n\n\n# 2. Encode target and categorical features\ndf[\"HeartDisease\"] = df[\"HeartDisease\"].map({\"Yes\": 1, \"No\": 0})\ndf_encoded = pd.get_dummies(df, drop_first=True)\n\n# 3. Drop target variable for unsupervised clustering\nX = df_encoded.drop(\"HeartDisease\", axis=1)\n\n\n# 4. Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 5. Elbow Method and Silhouette Scores\ninertia = []\nsil_scores = []\nK_range = range(2, 15)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertia.append(kmeans.inertia_)\n    sil_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\n\n# 6. Plot Elbow Method\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, inertia, marker='o')\nplt.title('Elbow Method: Optimal k')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.grid(True)\nplt.show()\n\n# 7. Plot Silhouette Scores\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, sil_scores, marker='o', color='green')\nplt.title('Silhouette Score for Each k')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 8. Apply KMeans with optimal k\noptimal_k = 4  # Set based on Elbow/Silhouette plot\nkmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ncluster_labels = kmeans_final.fit_predict(X_scaled)\ndf_encoded['Cluster'] = cluster_labels\n\n\n# 9. Export model and clustered data\nwith open(\"clustering_model.p\", \"wb\") as f:\n    pickle.dump(kmeans_final, f)\n\ndf_encoded.to_csv(\"heart_2020_clustered.csv\", index=False)"
  },
  {
    "objectID": "assets/drive-download-20250925T232328Z-1-001/9_ResultVisualization.html",
    "href": "assets/drive-download-20250925T232328Z-1-001/9_ResultVisualization.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "Result Visualization\n\nimport pyAgrum.skbn as skbn\nimport pyAgrum.lib.notebook as gnb\n\nfrom sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, RocCurveDisplay, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, RandomizedSearchCV\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport sample_data as sd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\nfrom sklearn.inspection import permutation_importance\n\nfrom IPython.display import display,HTML\ndf = pd.read_csv(\"/Users/NathanSwan 1/Downloads/heart_cleaned_final2.csv\", nrows=50000)\n\ndf.head()\nimport pickle\n\n# adjust paths/filenames as needed\nwith open('/Users/NathanSwan 1/Downloads/331 Project/X_train.p', 'rb') as f:\n    X_train = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/X_test.p', 'rb') as f:\n    X_test = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/y_train.p', 'rb') as f:\n    y_train = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/y_test.p', 'rb') as f:\n    y_test = pickle.load(f)\n\n# quick check\nprint(X_train.shape, X_test.shape, len(y_train), len(y_test))\n\n(35000, 21) (15000, 21) 35000 15000\n\n\n\nRidge Regression\n\n\nimport pickle\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport pickle\nridge_model_filename = \"ridge_model.pkl\"\nwith open(ridge_model_filename, 'rb') as file:\n    loaded_ridge_model = pickle.load(file)\nprint(\"\\nLoaded Ridge Model:\")\nprint(loaded_ridge_model)\n\n\n\nLoaded Ridge Model:\nRidge(alpha=7993.511705685619)\n\n\n\nRandom Forests\n\n\n\nimport pickle\n\n# Define the filename\nfeature_importance_filename = \"rf_feature_importances.p\"\n\n# Load the feature importances\nwith open(feature_importance_filename, \"rb\") as file:\n    loaded_importances = pickle.load(file)\n\nprint(\"Feature importances loaded successfully.\\n\")\n\nfeature_names = X_train.columns  # Adjust based on your dataset\n\n# Display with feature names\nfor name, importance in zip(feature_names, loaded_importances):\n    print(f\"{name}: {importance:.4f}\")\n\nFeature importances loaded successfully.\n\nBMI: 0.2338\nSmoking: 0.0148\nAlcoholDrinking: 0.0097\nStroke: 0.0263\nPhysicalHealth: 0.0569\nMentalHealth: 0.0485\nDiffWalking: 0.0261\nSex: 0.0308\nAgeCategory: 0.0228\nRace: 0.0283\nDiabetic: 0.0626\nPhysicalActivity: 0.0147\nGenHealth: 0.0134\nSleepTime: 0.0146\nAsthma: 0.0426\nKidneyDisease: 0.0589\nSkinCancer: 0.0141\nComorbidityCount: 0.0014\nUnhealthyDays: 0.0019\nRiskBehavior: 0.0024\nSleepCategory: 0.0031\n\n\n\n\n\nBMI: 0.2338\nSmoking: 0.0148\nAlcoholDrinking: 0.0097\nStroke: 0.0263\nPhysicalHealth: 0.0569\nMentalHealth: 0.0485\nDiffWalking: 0.0261\nSex: 0.0308\nAgeCategory: 0.0228\nRace: 0.0283\nDiabetic: 0.0626\nPhysicalActivity: 0.0147\nGenHealth: 0.0134\nSleepTime: 0.0146\nAsthma: 0.0426\nKidneyDisease: 0.0589\nSkinCancer: 0.0141\nComorbidityCount: 0.0014\nUnhealthyDays: 0.0019\nRiskBehavior: 0.0024\nSleepCategory: 0.0031\n\n\n\nBoosted Trees\n\n\nfrom google.colab import files\nuploaded = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving xgb_model2.pkl to xgb_model2.pkl\n\n\n\nimport pickle\n\nwith open(\"xgb_model2.pkl\", \"rb\") as f:\n    xgb_model = pickle.load(f)\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:21:07] WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\nconfiguration generated by an older version of XGBoost, please export the model by calling\n`Booster.save_model` from that version first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n\nfor more details about differences between saving model and serializing.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\nBayesian Network\n\n\nwith open('bayesian_network_model.pkl', 'rb') as file:\n    loaded_model = pickle.load(file)\ngnb.showBN(bnc.bn)\n\n\n\n\n\n\n\n\nBayesian Network: The bayesian network centralizes around HeartDisease and its variables that affect it. Some of the important key variables are AgeCategory, ComobidityCount, GenHealth, and Sex. If you go throw the network some things such as ComorbidityCount are influenced by many different variables. These variables are GenHealth, KidneyDisease, Stroke, Diabetic, and Asthma which would all make sense when talking about ComorbidityCount. Some other important connects are RiskBehavior with Smoking and AlcoholDrinking which also do make sense. Overall the network does show that alot of different variables are important to factor when analyzing HeartDisease. This model has an accuracy train/test accuracy aroud 73-74%.\n\nClustering\n\n\nimport pickle\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the necessary libraries\nimport numpy as np\nimport pandas as pd\n# Define the model filename\nclustering_model_filename = \"clustering_model.p\"\n\n# Open the file in read-binary mode and load the model\nwith open(clustering_model_filename, \"rb\") as file:\n    kmeans_model = pickle.load(file)\n\nprint(\"Clustering model successfully loaded.\")\n\nClustering model successfully loaded.\n\n\n\n# Display the clustering model\nprint(\"\\nLoaded Clustering Model:\")\nprint(kmeans_model)\n\n\nLoaded Clustering Model:\nKMeans(n_clusters=2, n_init=10, random_state=42)\n\n\n\nNeural Networks\n\n\nfrom google.colab import files\nuploaded = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving neural_net_model.p to neural_net_model.p\n\n\n\nimport pickle\nfrom tensorflow.keras.models import model_from_json\n\n# Define the wrapper class (must be included again in Colab)\nclass KerasModelWrapper:\n    def __init__(self, model):\n        self.model_json = model.to_json()\n        self.model_weights = model.get_weights()\n\n    def load_model(self):\n        model = model_from_json(self.model_json)\n        model.set_weights(self.model_weights)\n        return model\n\n# Load the model from the uploaded file\nwith open(\"neural_net_model.p\", \"rb\") as f:\n    wrapper = pickle.load(f)\n\nnn_model = wrapper.load_model()\nnn_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\n\nnn_model.summary()\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_3 (Dense)                 │ (None, 64)             │         2,944 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 32)             │         2,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 5,057 (19.75 KB)\n\n\n\n Trainable params: 5,057 (19.75 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nThe Neural Network was trained to predict the likelihood of heart disease using all available features in the dataset. One of the major challenges we faced was class imbalance — only about 9% of the data had heart disease cases, which led to very high accuracy but poor detection of actual disease cases in the baseline model.\nTo address this, we used class weighting, which forces the model to “care more” about the minority class (heart disease). After applying this, performance improved significantly on key metrics like recall and F1-score for the heart disease class.\nThe model architecture included:\nAn input layer connected to two hidden layers with ReLU activation\nA final output layer with a sigmoid activation for binary classification\nAfter training for 30 epochs, the model showed strong improvement:\nBaseline model: Accuracy ~92%, but Recall (1) was only 15%\nBalanced model: Accuracy ~76%, but Recall (1) increased to 79%\nThis trade-off shows that while overall accuracy dropped, the model became much better at identifying people with heart disease, which is the real-world priority. This model emphasizes how neural networks can be tuned to solve imbalanced classification problems effectively using weighting."
  },
  {
    "objectID": "assets/drive-download-20250925T232328Z-1-001/8_Neural_Network.html",
    "href": "assets/drive-download-20250925T232328Z-1-001/8_Neural_Network.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"heart_cleaned_final.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\n...\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\nComorbidityCount\nUnhealthyDays\nRiskBehavior\nSleepCategory\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n55-59\n...\n1\nVery good\n5.0\n1\n0\n1\n3\n30.0\n1\nVery Short\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n80 or older\n...\n1\nVery good\n7.0\n0\n0\n0\n1\n0.0\n0\nNormal\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n65-69\n...\n1\nFair\n8.0\n1\n0\n0\n2\n30.0\n1\nNormal\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n75-79\n...\n0\nGood\n6.0\n0\n0\n1\n1\n0.0\n0\nShort\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n40-44\n...\n1\nVery good\n8.0\n0\n0\n0\n0\n28.0\n0\nNormal\n\n\n\n\n5 rows × 22 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\n...\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\nComorbidityCount\nUnhealthyDays\nRiskBehavior\nSleepCategory\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n55-59\n...\n1\nVery good\n5.0\n1\n0\n1\n3\n30.0\n1\nVery Short\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n80 or older\n...\n1\nVery good\n7.0\n0\n0\n0\n1\n0.0\n0\nNormal\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n65-69\n...\n1\nFair\n8.0\n1\n0\n0\n2\n30.0\n1\nNormal\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n75-79\n...\n0\nGood\n6.0\n0\n0\n1\n1\n0.0\n0\nShort\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n40-44\n...\n1\nVery good\n8.0\n0\n0\n0\n0\n28.0\n0\nNormal\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n# Core data tools\nimport pandas as pd\nimport numpy as np\nimport pickle\n\n# Neural Network from TensorFlow (Keras)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Evaluation tools\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import class_weight\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf = pd.read_csv(\"heart_cleaned_final.csv\")\n\n# Load the pre-split data\nX_train = pickle.load(open(\"X_train.p\", \"rb\"))\nX_test = pickle.load(open(\"X_test.p\", \"rb\"))\ny_train = pickle.load(open(\"y_train.p\", \"rb\"))\ny_test = pickle.load(open(\"y_test.p\", \"rb\"))\n\n\n# Define the model architecture\nbaseline_model = Sequential()\nbaseline_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\nbaseline_model.add(Dense(32, activation='relu'))\nbaseline_model.add(Dense(1, activation='sigmoid'))  # Final layer: probability of heart disease\n\n# Compile the model\nbaseline_model.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy'])\n\nc:\\Users\\modyx\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\n# Fit the model\nbaseline_history = baseline_model.fit(X_train, y_train,\n                                      epochs=30,\n                                      batch_size=32,\n                                      validation_split=0.2,\n                                      verbose=1)\n\n\nEpoch 1/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9083 - loss: 0.2832 - val_accuracy: 0.9145 - val_loss: 0.2336\n\nEpoch 2/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9103 - loss: 0.2438 - val_accuracy: 0.9130 - val_loss: 0.2310\n\nEpoch 3/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9123 - loss: 0.2353 - val_accuracy: 0.9145 - val_loss: 0.2321\n\nEpoch 4/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9108 - loss: 0.2415 - val_accuracy: 0.9091 - val_loss: 0.2393\n\nEpoch 5/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9114 - loss: 0.2371 - val_accuracy: 0.9149 - val_loss: 0.2308\n\nEpoch 6/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9123 - loss: 0.2351 - val_accuracy: 0.9165 - val_loss: 0.2376\n\nEpoch 7/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9145 - loss: 0.2299 - val_accuracy: 0.9161 - val_loss: 0.2295\n\nEpoch 8/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9125 - loss: 0.2345 - val_accuracy: 0.9147 - val_loss: 0.2491\n\nEpoch 9/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9127 - loss: 0.2365 - val_accuracy: 0.9118 - val_loss: 0.2312\n\nEpoch 10/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 999us/step - accuracy: 0.9130 - loss: 0.2309 - val_accuracy: 0.9162 - val_loss: 0.2377\n\nEpoch 11/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9142 - loss: 0.2299 - val_accuracy: 0.9146 - val_loss: 0.2298\n\nEpoch 12/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 999us/step - accuracy: 0.9177 - loss: 0.2219 - val_accuracy: 0.9158 - val_loss: 0.2312\n\nEpoch 13/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9122 - loss: 0.2286 - val_accuracy: 0.9149 - val_loss: 0.2299\n\nEpoch 14/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 985us/step - accuracy: 0.9143 - loss: 0.2290 - val_accuracy: 0.9130 - val_loss: 0.2335\n\nEpoch 15/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9132 - loss: 0.2313 - val_accuracy: 0.9143 - val_loss: 0.2326\n\nEpoch 16/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9155 - loss: 0.2241 - val_accuracy: 0.9164 - val_loss: 0.2285\n\nEpoch 17/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 988us/step - accuracy: 0.9137 - loss: 0.2269 - val_accuracy: 0.9131 - val_loss: 0.2301\n\nEpoch 18/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 998us/step - accuracy: 0.9158 - loss: 0.2233 - val_accuracy: 0.9149 - val_loss: 0.2297\n\nEpoch 19/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 994us/step - accuracy: 0.9139 - loss: 0.2283 - val_accuracy: 0.9140 - val_loss: 0.2301\n\nEpoch 20/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 992us/step - accuracy: 0.9124 - loss: 0.2275 - val_accuracy: 0.9111 - val_loss: 0.2343\n\nEpoch 21/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9138 - loss: 0.2247 - val_accuracy: 0.9144 - val_loss: 0.2301\n\nEpoch 22/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 999us/step - accuracy: 0.9150 - loss: 0.2232 - val_accuracy: 0.9140 - val_loss: 0.2305\n\nEpoch 23/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 990us/step - accuracy: 0.9181 - loss: 0.2179 - val_accuracy: 0.9150 - val_loss: 0.2298\n\nEpoch 24/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 991us/step - accuracy: 0.9155 - loss: 0.2206 - val_accuracy: 0.9122 - val_loss: 0.2326\n\nEpoch 25/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 995us/step - accuracy: 0.9119 - loss: 0.2279 - val_accuracy: 0.9145 - val_loss: 0.2324\n\nEpoch 26/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9158 - loss: 0.2225 - val_accuracy: 0.9106 - val_loss: 0.2320\n\nEpoch 27/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 994us/step - accuracy: 0.9153 - loss: 0.2205 - val_accuracy: 0.9133 - val_loss: 0.2327\n\nEpoch 28/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9166 - loss: 0.2179 - val_accuracy: 0.9118 - val_loss: 0.2386\n\nEpoch 29/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9150 - loss: 0.2224 - val_accuracy: 0.9091 - val_loss: 0.2360\n\nEpoch 30/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 997us/step - accuracy: 0.9149 - loss: 0.2204 - val_accuracy: 0.9103 - val_loss: 0.2349\n\n\n\n\n\n# Predict class probabilities and convert to 0/1\ny_pred_prob = baseline_model.predict(X_test).flatten()\ny_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n# Classification report\nprint(\"📊 Classification Report (Baseline):\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion matrix\nplt.figure(figsize=(5, 4))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"No HD\", \"Yes HD\"], yticklabels=[\"No HD\", \"Yes HD\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix (Baseline NN)\")\nplt.tight_layout()\nplt.show()\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 0s 506us/step\n\n📊 Classification Report (Baseline):\n\n              precision    recall  f1-score   support\n\n\n\n           0       0.93      0.99      0.96      9174\n\n           1       0.48      0.15      0.23       826\n\n\n\n    accuracy                           0.92     10000\n\n   macro avg       0.70      0.57      0.59     10000\n\nweighted avg       0.89      0.92      0.90     10000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 4))\nplt.plot(baseline_history.history['accuracy'], label='Training Accuracy')\nplt.plot(baseline_history.history['val_accuracy'], label='Validation Accuracy')\nplt.title(\"Baseline NN Accuracy Over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nweights = class_weight.compute_class_weight(class_weight='balanced',\n                                             classes=np.unique(y_train),\n                                             y=y_train)\nclass_weights = {0: weights[0], 1: weights[1]}\nprint(\"Class Weights:\", class_weights)\n\nClass Weights: {0: 0.5483358008444371, 1: 5.672149744753262}\n\n\n\nweighted_model = Sequential()\nweighted_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\nweighted_model.add(Dense(32, activation='relu'))\nweighted_model.add(Dense(1, activation='sigmoid'))\n\nweighted_model.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy'])\n\nc:\\Users\\modyx\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\nweighted_history = weighted_model.fit(X_train, y_train,\n                                      epochs=30,\n                                      batch_size=32,\n                                      validation_split=0.2,\n                                      class_weight=class_weights,\n                                      verbose=1)\n\n\nEpoch 1/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.7152 - loss: 0.5922 - val_accuracy: 0.7216 - val_loss: 0.5484\n\nEpoch 2/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7394 - loss: 0.5171 - val_accuracy: 0.8593 - val_loss: 0.3162\n\nEpoch 3/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7399 - loss: 0.5154 - val_accuracy: 0.7150 - val_loss: 0.5535\n\nEpoch 4/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7327 - loss: 0.5103 - val_accuracy: 0.7136 - val_loss: 0.5528\n\nEpoch 5/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7458 - loss: 0.4904 - val_accuracy: 0.6879 - val_loss: 0.5949\n\nEpoch 6/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7362 - loss: 0.5057 - val_accuracy: 0.6770 - val_loss: 0.6056\n\nEpoch 7/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7352 - loss: 0.5008 - val_accuracy: 0.6840 - val_loss: 0.5854\n\nEpoch 8/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7424 - loss: 0.4842 - val_accuracy: 0.6565 - val_loss: 0.6620\n\nEpoch 9/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7354 - loss: 0.4931 - val_accuracy: 0.6645 - val_loss: 0.6050\n\nEpoch 10/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7365 - loss: 0.4765 - val_accuracy: 0.6998 - val_loss: 0.5677\n\nEpoch 11/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7369 - loss: 0.4797 - val_accuracy: 0.7184 - val_loss: 0.5361\n\nEpoch 12/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7350 - loss: 0.4815 - val_accuracy: 0.6965 - val_loss: 0.5610\n\nEpoch 13/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7355 - loss: 0.4837 - val_accuracy: 0.7511 - val_loss: 0.4821\n\nEpoch 14/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7447 - loss: 0.4747 - val_accuracy: 0.7423 - val_loss: 0.4979\n\nEpoch 15/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7289 - loss: 0.4841 - val_accuracy: 0.7271 - val_loss: 0.5178\n\nEpoch 16/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7312 - loss: 0.4911 - val_accuracy: 0.7886 - val_loss: 0.4270\n\nEpoch 17/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7502 - loss: 0.4717 - val_accuracy: 0.7341 - val_loss: 0.4948\n\nEpoch 18/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7386 - loss: 0.4792 - val_accuracy: 0.7529 - val_loss: 0.4799\n\nEpoch 19/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7429 - loss: 0.4737 - val_accuracy: 0.7315 - val_loss: 0.5027\n\nEpoch 20/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7347 - loss: 0.4808 - val_accuracy: 0.7659 - val_loss: 0.4455\n\nEpoch 21/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7426 - loss: 0.4778 - val_accuracy: 0.6425 - val_loss: 0.6431\n\nEpoch 22/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7346 - loss: 0.4747 - val_accuracy: 0.7401 - val_loss: 0.4866\n\nEpoch 23/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7474 - loss: 0.4722 - val_accuracy: 0.6854 - val_loss: 0.5816\n\nEpoch 24/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7410 - loss: 0.4655 - val_accuracy: 0.7502 - val_loss: 0.4979\n\nEpoch 25/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7387 - loss: 0.4751 - val_accuracy: 0.7086 - val_loss: 0.5334\n\nEpoch 26/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7386 - loss: 0.4625 - val_accuracy: 0.7045 - val_loss: 0.5512\n\nEpoch 27/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 996us/step - accuracy: 0.7435 - loss: 0.4693 - val_accuracy: 0.7349 - val_loss: 0.4896\n\nEpoch 28/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7478 - loss: 0.4556 - val_accuracy: 0.7351 - val_loss: 0.4982\n\nEpoch 29/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 993us/step - accuracy: 0.7462 - loss: 0.4669 - val_accuracy: 0.7370 - val_loss: 0.4889\n\nEpoch 30/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7461 - loss: 0.4556 - val_accuracy: 0.7456 - val_loss: 0.4749\n\n\n\n\n\n# Predict\ny_pred_prob = weighted_model.predict(X_test).flatten()\ny_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n# Report\nprint(\"📊 Classification Report (With Class Weights):\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\nplt.figure(figsize=(5, 4))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"No HD\", \"Yes HD\"], yticklabels=[\"No HD\", \"Yes HD\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix (Balanced NN)\")\nplt.tight_layout()\nplt.show()\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 0s 516us/step\n\n📊 Classification Report (With Class Weights):\n\n              precision    recall  f1-score   support\n\n\n\n           0       0.98      0.76      0.85      9174\n\n           1       0.23      0.79      0.35       826\n\n\n\n    accuracy                           0.76     10000\n\n   macro avg       0.60      0.77      0.60     10000\n\nweighted avg       0.91      0.76      0.81     10000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 4))\nplt.plot(weighted_history.history['accuracy'], label='Training Accuracy')\nplt.plot(weighted_history.history['val_accuracy'], label='Validation Accuracy')\nplt.title(\"NN Accuracy Over Epochs (With Class Weights)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "assets/drive-download-20250925T232328Z-1-001/5_BoostedTree.html",
    "href": "assets/drive-download-20250925T232328Z-1-001/5_BoostedTree.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "Step 1: Data Cleaning\n\nimport pandas as pd\ndf = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 18 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   HeartDisease      50000 non-null  object \n 1   BMI               50000 non-null  float64\n 2   Smoking           50000 non-null  object \n 3   AlcoholDrinking   50000 non-null  object \n 4   Stroke            50000 non-null  object \n 5   PhysicalHealth    50000 non-null  float64\n 6   MentalHealth      50000 non-null  float64\n 7   DiffWalking       50000 non-null  object \n 8   Sex               50000 non-null  object \n 9   AgeCategory       50000 non-null  object \n 10  Race              50000 non-null  object \n 11  Diabetic          50000 non-null  object \n 12  PhysicalActivity  50000 non-null  object \n 13  GenHealth         50000 non-null  object \n 14  SleepTime         50000 non-null  float64\n 15  Asthma            50000 non-null  object \n 16  KidneyDisease     50000 non-null  object \n 17  SkinCancer        50000 non-null  object \ndtypes: float64(4), object(14)\nmemory usage: 6.9+ MB\n\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\nRace\nDiabetic\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\n\n\n\n\n0\nNo\n16.60\nYes\nNo\nNo\n3.0\n30.0\nNo\nFemale\n55-59\nWhite\nYes\nYes\nVery good\n5.0\nYes\nNo\nYes\n\n\n1\nNo\n20.34\nNo\nNo\nYes\n0.0\n0.0\nNo\nFemale\n80 or older\nWhite\nNo\nYes\nVery good\n7.0\nNo\nNo\nNo\n\n\n2\nNo\n26.58\nYes\nNo\nNo\n20.0\n30.0\nNo\nMale\n65-69\nWhite\nYes\nYes\nFair\n8.0\nYes\nNo\nNo\n\n\n3\nNo\n24.21\nNo\nNo\nNo\n0.0\n0.0\nNo\nFemale\n75-79\nWhite\nNo\nNo\nGood\n6.0\nNo\nNo\nYes\n\n\n4\nNo\n23.71\nNo\nNo\nNo\n28.0\n0.0\nYes\nFemale\n40-44\nWhite\nNo\nYes\nVery good\n8.0\nNo\nNo\nNo\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n# Plotting the number of people with and without heart disease\nplt.figure(figsize=(8, 6))\ndf['HeartDisease'].value_counts().plot(kind='bar')\nplt.title('Number of People With and Without Heart Disease (First 50,000 Rows)')\nplt.xlabel('Heart Disease')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nbinary_cols = [\n    'HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke',\n    'DiffWalking', 'Diabetic', 'PhysicalActivity',\n    'Asthma', 'KidneyDisease', 'SkinCancer'\n]\n\ndf[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n/var/folders/gp/9b_fhx0s24s2954c8mqgvr240000gn/T/ipykernel_60282/918804490.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\n\ndf['Sex'] = df['Sex'].map({'Male': 1, 'Female': 0})\n\n\ndf['ComorbidityCount'] = df[['Stroke', 'Diabetic', 'Asthma', 'KidneyDisease', 'SkinCancer']].sum(axis=1)\n\n\ndf['UnhealthyDays'] = df['PhysicalHealth'] + df['MentalHealth']\ndf['UnhealthyDays'] = df['UnhealthyDays'].clip(upper=30)\n\n\ndf['RiskBehavior'] = ((df['Smoking'] == 1) | (df['AlcoholDrinking'] == 1)).astype(int)\n\n\ndf['SleepCategory'] = pd.cut(df['SleepTime'],\n                             bins=[0, 5, 6.9, 8.9, 24],\n                             labels=['Very Short', 'Short', 'Normal', 'Long'])\n\n\ndf = pd.get_dummies(df, columns = ['AgeCategory', 'Race', 'GenHealth', 'SleepCategory'], \n                    prefix = ['Age', 'Race', 'GenHealth', 'SleepCategory'], drop_first=False, dtype = int)\n\n\ndf.dtypes\n\nHeartDisease                             int64\nBMI                                    float64\nSmoking                                  int64\nAlcoholDrinking                          int64\nStroke                                   int64\nPhysicalHealth                         float64\nMentalHealth                           float64\nDiffWalking                              int64\nSex                                      int64\nDiabetic                                 int64\nPhysicalActivity                         int64\nSleepTime                              float64\nAsthma                                   int64\nKidneyDisease                            int64\nSkinCancer                               int64\nComorbidityCount                         int64\nUnhealthyDays                          float64\nRiskBehavior                             int64\nAge_18-24                                int64\nAge_25-29                                int64\nAge_30-34                                int64\nAge_35-39                                int64\nAge_40-44                                int64\nAge_45-49                                int64\nAge_50-54                                int64\nAge_55-59                                int64\nAge_60-64                                int64\nAge_65-69                                int64\nAge_70-74                                int64\nAge_75-79                                int64\nAge_80 or older                          int64\nRace_American Indian/Alaskan Native      int64\nRace_Asian                               int64\nRace_Black                               int64\nRace_Hispanic                            int64\nRace_Other                               int64\nRace_White                               int64\nGenHealth_Excellent                      int64\nGenHealth_Fair                           int64\nGenHealth_Good                           int64\nGenHealth_Poor                           int64\nGenHealth_Very good                      int64\nSleepCategory_Very Short                 int64\nSleepCategory_Short                      int64\nSleepCategory_Normal                     int64\nSleepCategory_Long                       int64\ndtype: object\n\n\n\ndf.to_csv(\"heart_cleaned_final.csv\", index=False)\n\n\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n\nX = df.drop(\"HeartDisease\", axis=1)  \ny = df[\"HeartDisease\"]              \n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Step 3: Save each set as a Pickle (.p) file\npickle.dump(X_train, open(\"X_train.p\", \"wb\"))\npickle.dump(X_test, open(\"X_test.p\", \"wb\"))\npickle.dump(y_train, open(\"y_train.p\", \"wb\"))\npickle.dump(y_test, open(\"y_test.p\", \"wb\"))\n\nprint(\"Pickle files saved successfully!\")\n\nPickle files saved successfully!\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV, RandomizedSearchCV\n\nfrom sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate\nimport numpy as np\nimport pandas as pd\nimport sample_data as sd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\nrs = 1234\n\n\nspw = (y_train.shape[0] - np.sum(y_train)) / np.sum(y_train)\nspw\n\n10.490479317137229\n\n\n\ndf.dtypes\n\nHeartDisease                             int64\nBMI                                    float64\nSmoking                                  int64\nAlcoholDrinking                          int64\nStroke                                   int64\nPhysicalHealth                         float64\nMentalHealth                           float64\nDiffWalking                              int64\nSex                                      int64\nDiabetic                                 int64\nPhysicalActivity                         int64\nSleepTime                              float64\nAsthma                                   int64\nKidneyDisease                            int64\nSkinCancer                               int64\nComorbidityCount                         int64\nUnhealthyDays                          float64\nRiskBehavior                             int64\nAge_18-24                                int64\nAge_25-29                                int64\nAge_30-34                                int64\nAge_35-39                                int64\nAge_40-44                                int64\nAge_45-49                                int64\nAge_50-54                                int64\nAge_55-59                                int64\nAge_60-64                                int64\nAge_65-69                                int64\nAge_70-74                                int64\nAge_75-79                                int64\nAge_80 or older                          int64\nRace_American Indian/Alaskan Native      int64\nRace_Asian                               int64\nRace_Black                               int64\nRace_Hispanic                            int64\nRace_Other                               int64\nRace_White                               int64\nGenHealth_Excellent                      int64\nGenHealth_Fair                           int64\nGenHealth_Good                           int64\nGenHealth_Poor                           int64\nGenHealth_Very good                      int64\nSleepCategory_Very Short                 int64\nSleepCategory_Short                      int64\nSleepCategory_Normal                     int64\nSleepCategory_Long                       int64\ndtype: object\n\n\n\nfrom xgboost import XGBClassifier ## There's more variety here as well XGBRegressor, XGBFClassifier, XGBFRegressor\n\nxg_cl = XGBClassifier(objective='binary:logistic', ## Objective function can be changed for specific case - there are many options!\n                        n_estimators = 100, ## Default is 100\n                        max_depth = 6, ## Default is 6\n                        eta = 0.3, ## AKA learning_rate, but XGB restricts it to [0,1], Default = 0.3,\n                        scale_pos_weight = spw, ## Treat the datata as balanced\n                        seed = rs) ## different name, same thing, random_state\n\nxg_cl.fit(X_train,y_train)\n\n#Predict the model - works the same\ny_train_hat = xg_cl.predict(X_train)\ny_test_hat = xg_cl.predict(X_test)\n\n## We do still have the 'score' function as in sklearn\nxg_cl.score(X_train, y_train), xg_cl.score(X_test, y_test) ## we set `binary:logisitc` on a Classifier, so score is Accuracy\n\n(0.8542857142857143, 0.7976666666666666)\n\n\n\n## Base\nclf = XGBClassifier(objective='binary:logistic', scale_pos_weight = spw, seed = rs) \n\n## KFold Stratified\nfrom sklearn.model_selection import StratifiedKFold\ncv_5 = StratifiedKFold(n_splits=5)\n\n## Parameter Grid\nparam_grid = {'max_depth': np.arange(5,12,1),\n              'n_estimators': np.arange(75,100,5),\n              'eta': np.arange(.3,.5,.1),\n              'min_child_weight': [1, 2, 3],\n            'scale_pos_weight': [1, 2, 3]}\n\n## Run the Grid search\nsh = RandomizedSearchCV(clf,\n                         param_grid,\n                         n_iter = 50,\n                         cv = cv_5,\n                         random_state = rs,).fit(X_train, y_train)\nsh.best_estimator_\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eta=0.4, eval_metric=None,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n              max_leaves=None, min_child_weight=1, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=75,\n              n_jobs=None, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriFittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eta=0.4, eval_metric=None,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n              max_leaves=None, min_child_weight=1, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=75,\n              n_jobs=None, num_parallel_tree=None, ...) \n\n\n\nxg_cl = sh.best_estimator_\n\n#Predict the model - works the same\ny_train_hat = xg_cl.predict(X_train)\ny_test_hat = xg_cl.predict(X_test)\n\n## We do still have the 'score' function as in sklearn\nxg_cl.score(X_train, y_train), xg_cl.score(X_test, y_test)\n\n(0.9293428571428571, 0.9118666666666667)\n\n\n\nlabels_ = ['No Disease', 'Disease']\n\ncm_train = confusion_matrix(y_train, y_train_hat, normalize='true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=labels_)\ndisp.plot()\nplt.title(\"Predicting Heart Disease by XGB Train Set\")\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\ncm_test = confusion_matrix(y_test, y_test_hat, normalize='true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels= labels_)\ndisp.plot()\nplt.title(\"Predicting Heart Disease by XGB Test Set\")\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import f1_score, roc_auc_score, roc_curve\n\n\n# Predict on test set\ny_pred = xg_cl.predict(X_test)\n\n# Evaluate\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Print results\nprint(f\"Accuracy:  {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall:    {recall:.2f}\")\n\nAccuracy:  0.91\nPrecision: 0.48\nRecall:    0.14\n\n\n\n# Predict probabilities for ROC AUC\ny_proba = xg_cl.predict_proba(X_test)[:, 1]  # Probability of class 1\n\n# Compute metrics\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nprint(f\"F1 Score:  {f1:.2f}\")\nprint(f\"ROC AUC:   {roc_auc:.2f}\")\n\nF1 Score:  0.22\nROC AUC:   0.82\n\n\nThe model has high accuracy (0.91) but low recall (0.14), meaning it misses most heart disease cases. Precision (0.48) is also low, leading to false positives. The F1 score (0.22) reflects poor balance between precision and recall, while the ROC AUC (0.82) shows good overall class differentiation. Improving recall is crucial to avoid missing heart disease cases. Probable Cause: Dataset Class Imbalance between NoHeartDisease/HeartDisease respectively,(45,648/4,352)\n\ndf_tree = xg_cl.get_booster().trees_to_dataframe()\ndf_tree.head(10)\n\n\n\n\n\n\n\n\nTree\nNode\nID\nFeature\nSplit\nYes\nNo\nMissing\nGain\nCover\nCategory\n\n\n\n\n0\n0\n0\n0-0\nComorbidityCount\n2.0\n0-1\n0-2\n0-2\n976.216797\n4724.306150\nNaN\n\n\n1\n0\n1\n0-1\nDiffWalking\n1.0\n0-3\n0-4\n0-4\n419.088623\n4347.576170\nNaN\n\n\n2\n0\n2\n0-2\nStroke\n1.0\n0-5\n0-6\n0-6\n129.842499\n376.729645\nNaN\n\n\n3\n0\n3\n0-3\nAge_80 or older\n1.0\n0-7\n0-8\n0-8\n149.806885\n3830.737300\nNaN\n\n\n4\n0\n4\n0-4\nStroke\n1.0\n0-9\n0-10\n0-10\n65.573822\n516.839050\nNaN\n\n\n5\n0\n5\n0-5\nDiffWalking\n1.0\n0-11\n0-12\n0-12\n69.850998\n268.205597\nNaN\n\n\n6\n0\n6\n0-6\nGenHealth_Very good\n1.0\n0-13\n0-14\n0-14\n33.572876\n108.524055\nNaN\n\n\n7\n0\n7\n0-7\nComorbidityCount\n1.0\n0-15\n0-16\n0-16\n105.684814\n3599.786130\nNaN\n\n\n8\n0\n8\n0-8\nSex\n1.0\n0-17\n0-18\n0-18\n35.201023\n230.951080\nNaN\n\n\n9\n0\n9\n0-9\nGenHealth_Poor\n1.0\n0-19\n0-20\n0-20\n52.831093\n489.573090\nNaN\n\n\n\n\n\n\n\n\ndf_tree.shape\n\n(3841, 11)\n\n\n\n## tells us how many splits were done \ndf_tree[df_tree['Feature'] != 'Leaf'].shape[0]\n\n1883\n\n\n\ndf_tree_features = df_tree[df_tree['Feature'] != 'Leaf'].copy()\ndf_tree_features\n\n\n\n\n\n\n\n\nTree\nNode\nID\nFeature\nSplit\nYes\nNo\nMissing\nGain\nCover\nCategory\n\n\n\n\n0\n0\n0\n0-0\nComorbidityCount\n2.000000\n0-1\n0-2\n0-2\n976.216797\n4724.306150\nNaN\n\n\n1\n0\n1\n0-1\nDiffWalking\n1.000000\n0-3\n0-4\n0-4\n419.088623\n4347.576170\nNaN\n\n\n2\n0\n2\n0-2\nStroke\n1.000000\n0-5\n0-6\n0-6\n129.842499\n376.729645\nNaN\n\n\n3\n0\n3\n0-3\nAge_80 or older\n1.000000\n0-7\n0-8\n0-8\n149.806885\n3830.737300\nNaN\n\n\n4\n0\n4\n0-4\nStroke\n1.000000\n0-9\n0-10\n0-10\n65.573822\n516.839050\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3808\n74\n24\n74-24\nBMI\n36.580002\n74-47\n74-48\n74-48\n0.732548\n4.485965\nNaN\n\n\n3809\n74\n25\n74-25\nGenHealth_Very good\n1.000000\n74-49\n74-50\n74-50\n1.045059\n4.408062\nNaN\n\n\n3810\n74\n26\n74-26\nSex\n1.000000\n74-51\n74-52\n74-52\n3.796722\n8.525488\nNaN\n\n\n3811\n74\n27\n74-27\nMentalHealth\n5.000000\n74-53\n74-54\n74-54\n3.579046\n15.346827\nNaN\n\n\n3812\n74\n28\n74-28\nSleepTime\n8.000000\n74-55\n74-56\n74-56\n1.515866\n5.987767\nNaN\n\n\n\n\n1883 rows × 11 columns\n\n\n\n\ndf_import = df_tree_features[['Feature','Gain']].copy()\n\n## We can look at summary stats using .groupby()\n## Let's look at mean, max, and quantile(0.95)\ndf_import.groupby('Feature')[['Gain']].quantile(0.95).sort_values(by = 'Gain',ascending=False).reset_index()\n\n\n\n\n\n\n\n\nFeature\nGain\n\n\n\n\n0\nDiffWalking\n115.867584\n\n\n1\nAge_80 or older\n91.714090\n\n\n2\nGenHealth_Excellent\n91.030835\n\n\n3\nStroke\n71.457689\n\n\n4\nSmoking\n49.810649\n\n\n5\nAge_25-29\n44.986475\n\n\n6\nGenHealth_Good\n44.838613\n\n\n7\nAge_18-24\n40.519531\n\n\n8\nAge_35-39\n40.496554\n\n\n9\nAge_75-79\n40.467848\n\n\n10\nAge_30-34\n38.670660\n\n\n11\nSex\n36.303448\n\n\n12\nGenHealth_Poor\n35.768389\n\n\n13\nAge_40-44\n35.579798\n\n\n14\nGenHealth_Very good\n33.748779\n\n\n15\nAsthma\n31.112913\n\n\n16\nComorbidityCount\n26.316109\n\n\n17\nAge_45-49\n25.740568\n\n\n18\nAge_70-74\n25.322577\n\n\n19\nRace_White\n18.587953\n\n\n20\nAge_65-69\n14.936148\n\n\n21\nAge_50-54\n12.706296\n\n\n22\nDiabetic\n11.539001\n\n\n23\nRace_Other\n11.461710\n\n\n24\nKidneyDisease\n9.889219\n\n\n25\nSkinCancer\n8.241578\n\n\n26\nSleepTime\n7.526331\n\n\n27\nAge_60-64\n7.401216\n\n\n28\nGenHealth_Fair\n7.102598\n\n\n29\nSleepCategory_Short\n7.066956\n\n\n30\nPhysicalHealth\n6.997932\n\n\n31\nBMI\n6.637537\n\n\n32\nUnhealthyDays\n6.189965\n\n\n33\nPhysicalActivity\n5.800232\n\n\n34\nMentalHealth\n5.638998\n\n\n35\nAge_55-59\n5.616385\n\n\n36\nRace_Hispanic\n5.203843\n\n\n37\nRace_Black\n5.084721\n\n\n38\nSleepCategory_Normal\n4.828359\n\n\n39\nAlcoholDrinking\n4.588219\n\n\n40\nRiskBehavior\n4.120059\n\n\n41\nRace_American Indian/Alaskan Native\n3.736672\n\n\n42\nRace_Asian\n2.006217\n\n\n\n\n\n\n\n\norder_ = df_import.groupby('Feature')['Gain'].mean().sort_values(ascending=False).nlargest(20).index\n\nsns.barplot(data = df_import,\n            y = 'Feature',\n            x = 'Gain', \n            errorbar = 'sd', \n            order = order_);\nplt.title(\"Feature Gain of XGB\");\n\n\n\n\n\n\n\n\n\ndf_tree_features.groupby(['Feature'])[['Split']].count().reset_index().sort_values(\"Split\", ascending = False)\n\n\n\n\n\n\n\n\nFeature\nSplit\n\n\n\n\n15\nBMI\n463\n\n\n39\nSleepTime\n142\n\n\n27\nPhysicalHealth\n117\n\n\n42\nUnhealthyDays\n111\n\n\n25\nMentalHealth\n106\n\n\n16\nComorbidityCount\n88\n\n\n35\nSex\n58\n\n\n23\nGenHealth_Very good\n41\n\n\n26\nPhysicalActivity\n38\n\n\n41\nStroke\n37\n\n\n17\nDiabetic\n37\n\n\n40\nSmoking\n34\n\n\n14\nAsthma\n33\n\n\n18\nDiffWalking\n33\n\n\n33\nRace_White\n32\n\n\n12\nAge_80 or older\n31\n\n\n38\nSleepCategory_Short\n29\n\n\n20\nGenHealth_Fair\n28\n\n\n19\nGenHealth_Excellent\n27\n\n\n31\nRace_Hispanic\n25\n\n\n21\nGenHealth_Good\n25\n\n\n9\nAge_65-69\n25\n\n\n24\nKidneyDisease\n24\n\n\n30\nRace_Black\n24\n\n\n36\nSkinCancer\n21\n\n\n37\nSleepCategory_Normal\n21\n\n\n8\nAge_60-64\n21\n\n\n11\nAge_75-79\n20\n\n\n10\nAge_70-74\n20\n\n\n22\nGenHealth_Poor\n18\n\n\n0\nAge_18-24\n15\n\n\n5\nAge_45-49\n15\n\n\n6\nAge_50-54\n14\n\n\n13\nAlcoholDrinking\n13\n\n\n7\nAge_55-59\n13\n\n\n34\nRiskBehavior\n13\n\n\n1\nAge_25-29\n12\n\n\n28\nRace_American Indian/Alaskan Native\n12\n\n\n2\nAge_30-34\n11\n\n\n32\nRace_Other\n11\n\n\n4\nAge_40-44\n10\n\n\n3\nAge_35-39\n10\n\n\n29\nRace_Asian\n5\n\n\n\n\n\n\n\n\n## The Histplot is a lot easier now actually from the df_features table\n\n## I still need to force an ordering on the Variable Column to \n## organize the HistPlot\n## I will resuse the `order_` from before\ndf_import['Feauture'] = pd.Categorical(df_import['Feature'],\n                                       ordered = True,\n                                       categories = order_)\n\nsns.histplot(data = df_import[df_import['Gain'] &lt; 1000],\n            y = 'Feature',\n            x = 'Gain');\nplt.title(\"Feature Gain of XGB\");\n\n\n\n\n\n\n\n\n\nwith open('xgb_model2.pkl', 'wb') as f:\n    pickle.dump(xg_cl, f)"
  },
  {
    "objectID": "assets/1_EDA_Cleaning.html",
    "href": "assets/1_EDA_Cleaning.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\ndf.info()\ndf.head()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-1-6da2b58110ee&gt; in &lt;cell line: 0&gt;()\n      1 import pandas as pd\n----&gt; 2 df = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\n      3 df.info()\n      4 df.head()\n\n/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1024     kwds.update(kwds_defaults)\n   1025 \n-&gt; 1026     return _read(filepath_or_buffer, kwds)\n   1027 \n   1028 \n\n/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py in _read(filepath_or_buffer, kwds)\n    618 \n    619     # Create the parser.\n--&gt; 620     parser = TextFileReader(filepath_or_buffer, **kwds)\n    621 \n    622     if chunksize or iterator:\n\n/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py in __init__(self, f, engine, **kwds)\n   1618 \n   1619         self.handles: IOHandles | None = None\n-&gt; 1620         self._engine = self._make_engine(f, self.engine)\n   1621 \n   1622     def close(self) -&gt; None:\n\n/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py in _make_engine(self, f, engine)\n   1878                 if \"b\" not in mode:\n   1879                     mode += \"b\"\n-&gt; 1880             self.handles = get_handle(\n   1881                 f,\n   1882                 mode,\n\n/usr/local/lib/python3.11/dist-packages/pandas/io/common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    871         if ioargs.encoding and \"b\" not in ioargs.mode:\n    872             # Encoding\n--&gt; 873             handle = open(\n    874                 handle,\n    875                 ioargs.mode,\n\nFileNotFoundError: [Errno 2] No such file or directory: 'heart_2020_cleaned.csv'\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n# Plotting the number of people with and without heart disease\nplt.figure(figsize=(8, 6))\ndf['HeartDisease'].value_counts().plot(kind='bar')\nplt.title('Number of People With and Without Heart Disease (First 50,000 Rows)')\nplt.xlabel('Heart Disease')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nBMI\nPhysicalHealth\nMentalHealth\nSleepTime\n\n\n\n\ncount\n50000.000000\n50000.000000\n50000.000000\n50000.00000\n\n\nmean\n27.971388\n3.539560\n3.984260\n7.12938\n\n\nstd\n6.239799\n8.094921\n7.979439\n1.49613\n\n\nmin\n12.400000\n0.000000\n0.000000\n1.00000\n\n\n25%\n23.710000\n0.000000\n0.000000\n6.00000\n\n\n50%\n26.960000\n0.000000\n0.000000\n7.00000\n\n\n75%\n31.010000\n2.000000\n4.000000\n8.00000\n\n\nmax\n87.050000\n30.000000\n30.000000\n24.00000\n\n\n\n\n\n\n\n\nbinary_cols = [\n    'HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke',\n    'DiffWalking', 'Diabetic', 'PhysicalActivity',\n    'Asthma', 'KidneyDisease', 'SkinCancer'\n]\n\ndf[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\nC:\\Users\\modyx\\AppData\\Local\\Temp\\ipykernel_21892\\1360497000.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\nRace\nDiabetic\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\nFemale\n55-59\nWhite\n1\n1\nVery good\n5.0\n1\n0\n1\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\nFemale\n80 or older\nWhite\n0\n1\nVery good\n7.0\n0\n0\n0\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\nMale\n65-69\nWhite\n1\n1\nFair\n8.0\n1\n0\n0\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\nFemale\n75-79\nWhite\n0\n0\nGood\n6.0\n0\n0\n1\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\nFemale\n40-44\nWhite\n0\n1\nVery good\n8.0\n0\n0\n0\n\n\n\n\n\n\n\n\n## Male is  1, Female is 0\ndf['Sex'] = df['Sex'].map({'Male': 1, 'Female': 0})\n\n\ndf['ComorbidityCount'] = df[['Stroke', 'Diabetic', 'Asthma', 'KidneyDisease', 'SkinCancer']].sum(axis=1)\n\n\ndf['UnhealthyDays'] = df['PhysicalHealth'] + df['MentalHealth']\ndf['UnhealthyDays'] = df['UnhealthyDays'].clip(upper=30)\n\n\ndf['RiskBehavior'] = ((df['Smoking'] == 1) | (df['AlcoholDrinking'] == 1)).astype(int)\n\n\ndf['SleepCategory'] = pd.cut(df['SleepTime'],\n                             bins=[0, 5, 6.9, 8.9, 24],\n                             labels=['Very Short', 'Short', 'Normal', 'Long'])\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\n...\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\nComorbidityCount\nUnhealthyDays\nRiskBehavior\nSleepCategory\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n55-59\n...\n1\nVery good\n5.0\n1\n0\n1\n3\n30.0\n1\nVery Short\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n80 or older\n...\n1\nVery good\n7.0\n0\n0\n0\n1\n0.0\n0\nNormal\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n65-69\n...\n1\nFair\n8.0\n1\n0\n0\n2\n30.0\n1\nNormal\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n75-79\n...\n0\nGood\n6.0\n0\n0\n1\n1\n0.0\n0\nShort\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n40-44\n...\n1\nVery good\n8.0\n0\n0\n0\n0\n28.0\n0\nNormal\n\n\n\n\n5 rows × 22 columns\n\n\n\n\ndf.columns\n\nIndex(['HeartDisease', 'BMI', 'Smoking', 'AlcoholDrinking', 'Stroke',\n       'PhysicalHealth', 'MentalHealth', 'DiffWalking', 'Sex', 'AgeCategory',\n       'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth', 'SleepTime',\n       'Asthma', 'KidneyDisease', 'SkinCancer', 'ComorbidityCount',\n       'UnhealthyDays', 'RiskBehavior', 'SleepCategory'],\n      dtype='object')\n\n\n\ndf.to_csv(\"heart_cleaned_final.csv\", index=False)\n\n\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n\nX = df.drop(\"HeartDisease\", axis=1)\ny = df[\"HeartDisease\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Step 3: Save each set as a Pickle (.p) file\npickle.dump(X_train, open(\"X_train.p\", \"wb\"))\npickle.dump(X_test, open(\"X_test.p\", \"wb\"))\npickle.dump(y_train, open(\"y_train.p\", \"wb\"))\npickle.dump(y_test, open(\"y_test.p\", \"wb\"))\n\nprint(\"Pickle files saved successfully!\")\n\nPickle files saved successfully!"
  },
  {
    "objectID": "assets/4_Random Forests.html",
    "href": "assets/4_Random Forests.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\ndf.info()\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 18 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   HeartDisease      50000 non-null  object \n 1   BMI               50000 non-null  float64\n 2   Smoking           50000 non-null  object \n 3   AlcoholDrinking   50000 non-null  object \n 4   Stroke            50000 non-null  object \n 5   PhysicalHealth    50000 non-null  float64\n 6   MentalHealth      50000 non-null  float64\n 7   DiffWalking       50000 non-null  object \n 8   Sex               50000 non-null  object \n 9   AgeCategory       50000 non-null  object \n 10  Race              50000 non-null  object \n 11  Diabetic          50000 non-null  object \n 12  PhysicalActivity  50000 non-null  object \n 13  GenHealth         50000 non-null  object \n 14  SleepTime         50000 non-null  float64\n 15  Asthma            50000 non-null  object \n 16  KidneyDisease     50000 non-null  object \n 17  SkinCancer        50000 non-null  object \ndtypes: float64(4), object(14)\nmemory usage: 6.9+ MB\n\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\nRace\nDiabetic\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\n\n\n\n\n0\nNo\n16.60\nYes\nNo\nNo\n3.0\n30.0\nNo\nFemale\n55-59\nWhite\nYes\nYes\nVery good\n5.0\nYes\nNo\nYes\n\n\n1\nNo\n20.34\nNo\nNo\nYes\n0.0\n0.0\nNo\nFemale\n80 or older\nWhite\nNo\nYes\nVery good\n7.0\nNo\nNo\nNo\n\n\n2\nNo\n26.58\nYes\nNo\nNo\n20.0\n30.0\nNo\nMale\n65-69\nWhite\nYes\nYes\nFair\n8.0\nYes\nNo\nNo\n\n\n3\nNo\n24.21\nNo\nNo\nNo\n0.0\n0.0\nNo\nFemale\n75-79\nWhite\nNo\nNo\nGood\n6.0\nNo\nNo\nYes\n\n\n4\nNo\n23.71\nNo\nNo\nNo\n28.0\n0.0\nYes\nFemale\n40-44\nWhite\nNo\nYes\nVery good\n8.0\nNo\nNo\nNo\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n# Plotting the number of people with and without heart disease\nplt.figure(figsize=(8, 6))\ndf['HeartDisease'].value_counts().plot(kind='bar')\nplt.title('Number of People With and Without Heart Disease (First 50,000 Rows)')\nplt.xlabel('Heart Disease')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nBMI\nPhysicalHealth\nMentalHealth\nSleepTime\n\n\n\n\ncount\n50000.000000\n50000.000000\n50000.000000\n50000.00000\n\n\nmean\n27.971388\n3.539560\n3.984260\n7.12938\n\n\nstd\n6.239799\n8.094921\n7.979439\n1.49613\n\n\nmin\n12.400000\n0.000000\n0.000000\n1.00000\n\n\n25%\n23.710000\n0.000000\n0.000000\n6.00000\n\n\n50%\n26.960000\n0.000000\n0.000000\n7.00000\n\n\n75%\n31.010000\n2.000000\n4.000000\n8.00000\n\n\nmax\n87.050000\n30.000000\n30.000000\n24.00000\n\n\n\n\n\n\n\n\nbinary_cols = [\n    'HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke',\n    'DiffWalking', 'Diabetic', 'PhysicalActivity',\n    'Asthma', 'KidneyDisease', 'SkinCancer'\n]\n\ndf[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\nC:\\Users\\modyx\\AppData\\Local\\Temp\\ipykernel_21892\\1360497000.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\nRace\nDiabetic\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\nFemale\n55-59\nWhite\n1\n1\nVery good\n5.0\n1\n0\n1\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\nFemale\n80 or older\nWhite\n0\n1\nVery good\n7.0\n0\n0\n0\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\nMale\n65-69\nWhite\n1\n1\nFair\n8.0\n1\n0\n0\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\nFemale\n75-79\nWhite\n0\n0\nGood\n6.0\n0\n0\n1\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\nFemale\n40-44\nWhite\n0\n1\nVery good\n8.0\n0\n0\n0\n\n\n\n\n\n\n\n\n## Male is  1, Female is 0\ndf['Sex'] = df['Sex'].map({'Male': 1, 'Female': 0})\n\n\ndf['ComorbidityCount'] = df[['Stroke', 'Diabetic', 'Asthma', 'KidneyDisease', 'SkinCancer']].sum(axis=1)\n\n\ndf['UnhealthyDays'] = df['PhysicalHealth'] + df['MentalHealth']\ndf['UnhealthyDays'] = df['UnhealthyDays'].clip(upper=30)\n\n\ndf['RiskBehavior'] = ((df['Smoking'] == 1) | (df['AlcoholDrinking'] == 1)).astype(int)\n\n\ndf['SleepCategory'] = pd.cut(df['SleepTime'],\n                             bins=[0, 5, 6.9, 8.9, 24],\n                             labels=['Very Short', 'Short', 'Normal', 'Long'])\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\n...\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\nComorbidityCount\nUnhealthyDays\nRiskBehavior\nSleepCategory\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n55-59\n...\n1\nVery good\n5.0\n1\n0\n1\n3\n30.0\n1\nVery Short\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n80 or older\n...\n1\nVery good\n7.0\n0\n0\n0\n1\n0.0\n0\nNormal\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n65-69\n...\n1\nFair\n8.0\n1\n0\n0\n2\n30.0\n1\nNormal\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n75-79\n...\n0\nGood\n6.0\n0\n0\n1\n1\n0.0\n0\nShort\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n40-44\n...\n1\nVery good\n8.0\n0\n0\n0\n0\n28.0\n0\nNormal\n\n\n\n\n5 rows × 22 columns\n\n\n\n\ndf.columns\n\nIndex(['HeartDisease', 'BMI', 'Smoking', 'AlcoholDrinking', 'Stroke',\n       'PhysicalHealth', 'MentalHealth', 'DiffWalking', 'Sex', 'AgeCategory',\n       'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth', 'SleepTime',\n       'Asthma', 'KidneyDisease', 'SkinCancer', 'ComorbidityCount',\n       'UnhealthyDays', 'RiskBehavior', 'SleepCategory'],\n      dtype='object')\n\n\n\ndf.to_csv(\"heart_cleaned_final.csv\", index=False)\n\n\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n\nX = df.drop(\"HeartDisease\", axis=1)\ny = df[\"HeartDisease\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Step 3: Save each set as a Pickle (.p) file\npickle.dump(X_train, open(\"X_train.p\", \"wb\"))\npickle.dump(X_test, open(\"X_test.p\", \"wb\"))\npickle.dump(y_train, open(\"y_train.p\", \"wb\"))\npickle.dump(y_test, open(\"y_test.p\", \"wb\"))\n\nprint(\"Pickle files saved successfully!\")\n\nPickle files saved successfully!\n\n\n\n# 1. Imports\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport matplotlib.pyplot as plt\n\n# 2. Load and preprocess the data\ndf = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\n\n# Convert binary columns\nbinary_cols = [\n    'HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke',\n    'DiffWalking', 'Diabetic', 'PhysicalActivity',\n    'Asthma', 'KidneyDisease', 'SkinCancer'\n]\ndf[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n# Convert Sex\ndf['Sex'] = df['Sex'].map({'Male': 1, 'Female': 0})\n\n# Feature engineering\ndf['ComorbidityCount'] = df[['Stroke', 'Diabetic', 'Asthma', 'KidneyDisease', 'SkinCancer']].sum(axis=1)\ndf['UnhealthyDays'] = (df['PhysicalHealth'] + df['MentalHealth']).clip(upper=30)\ndf['RiskBehavior'] = ((df['Smoking'] == 1) | (df['AlcoholDrinking'] == 1)).astype(int)\ndf['SleepCategory'] = pd.cut(df['SleepTime'],\n                             bins=[0, 5, 6.9, 8.9, 24],\n                             labels=['Very Short', 'Short', 'Normal', 'Long'])\n\n# One-hot encode all categorical (object) features\ndf = pd.get_dummies(df)\n\n# Drop rows with missing values\ndf = df.dropna()\n\n# 3. Train-test split\nX = df.drop(\"HeartDisease\", axis=1)\ny = df[\"HeartDisease\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# 4. Train Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# 5. Evaluate model\ny_pred = rf_model.predict(X_test)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\nprint(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n\n# 6. Feature Importance Plot\nimportances = rf_model.feature_importances_\nfeatures = X_train.columns\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(12, 6))\nplt.title(\"Feature Importances (Random Forest)\")\nplt.bar(range(len(importances)), importances[indices])\nplt.xticks(range(len(importances)), features[indices], rotation=90)\nplt.tight_layout()\nplt.grid(axis='y')\nplt.show()\n\n# 7. Save model and data splits\npickle.dump(rf_model, open(\"random_forest_model.p\", \"wb\"))\npickle.dump(X_train, open(\"X_train.p\", \"wb\"))\npickle.dump(X_test, open(\"X_test.p\", \"wb\"))\npickle.dump(y_train, open(\"y_train.p\", \"wb\"))\npickle.dump(y_test, open(\"y_test.p\", \"wb\"))\n\n\n\n/var/folders/66/l7dnx1g128b7j804pfwmzc480000gn/T/ipykernel_32451/4278937313.py:19: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\nConfusion Matrix:\n[[13455   239]\n [ 1164   142]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.92      0.98      0.95     13694\n           1       0.37      0.11      0.17      1306\n\n    accuracy                           0.91     15000\n   macro avg       0.65      0.55      0.56     15000\nweighted avg       0.87      0.91      0.88     15000\n\n\nAccuracy Score: 0.9064666666666666"
  },
  {
    "objectID": "assets/3_Ridge Regression.html",
    "href": "assets/3_Ridge Regression.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load dataset\ndf = pd.read_csv(r\"C:\\Users\\Michael Antonucci\\OneDrive\\Documents\\MISY331 Final Project\\heart_cleaned_final.csv\")\n\n# Convert categorical columns to numeric\ndf_encoded = pd.get_dummies(df, drop_first=True)\n\n# Define features and target\nX = df_encoded.drop(\"HeartDisease\", axis=1)\ny = df_encoded[\"HeartDisease\"]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# Normalize features\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Range of alpha values\nalphas = np.linspace(1, 10000, 300)\n\ncoefs = []\n\n# Fit Ridge regression for each alpha\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_train_scaled, y_train)\n    coefs.append(ridge.coef_)\n\ncoefs = np.array(coefs)\n\n# Evaluate Ridge model at alpha ~ 8000\nalpha_index = np.abs(alphas - 8000).argmin()\nridge_final = Ridge(alpha=alphas[alpha_index])\nridge_final.fit(X_train_scaled, y_train)\n\n# Predictions and performance\ny_pred = ridge_final.predict(X_test_scaled)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Ridge Regression at alpha={alphas[alpha_index]:.2f}\")\nprint(f\"Mean Squared Error: {mse:.4f}\")\nprint(f\"R-squared: {r2:.4f}\")\n\n# Top 5 most influential features at alpha ~8000\ntop_5_indices = np.argsort(np.abs(coefs[alpha_index]))[-5:]\ntop_5_features = np.array(X.columns)[top_5_indices]\nprint(\"Top 5 important features at alpha ≈ 8000:\", top_5_features)\n\n# Spaghetti plot for top 5 features\nplt.figure(figsize=(10, 6))\nfor i in top_5_indices:\n    plt.plot(alphas, coefs[:, i], label=X.columns[i])\n\nplt.xscale(\"log\")\nplt.xlabel(\"Alpha (log scale)\")\nplt.ylabel(\"Coefficient Value\")\nplt.title(\"Ridge Coefficient Paths for Top 5 Features\")\nplt.legend()\n\nRidge Regression at alpha=7993.51\nMean Squared Error: 0.0680\nR-squared: 0.1129\nTop 5 important features at alpha ≈ 8000: ['GenHealth_Poor' 'Age_80 or older' 'DiffWalking' 'Stroke' 'Diabetic']\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n\n# Prepare your data\nX = df_encoded.drop(\"HeartDisease\", axis=1)\ny = df_encoded[\"HeartDisease\"]\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit Ridge Regression model\nridge = Ridge(alpha=1.0)  # You can tune alpha\nridge.fit(X_train, y_train)\n\n# Predict on test set\ny_pred_continuous = ridge.predict(X_test)\n\n# Convert continuous output to binary labels (threshold at 0.5)\ny_pred_binary = (y_pred_continuous &gt;= 0.5).astype(int)\n\n# Evaluate metrics\naccuracy = accuracy_score(y_test, y_pred_binary)\nprecision = precision_score(y_test, y_pred_binary)\nrecall = recall_score(y_test, y_pred_binary)\ncm = confusion_matrix(y_test, y_pred_binary)\n\n# Print results\nprint(\"Ridge Regression Evaluation Metrics:\")\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\nRidge Regression Evaluation Metrics:\nAccuracy : 0.9189\nPrecision: 0.6154\nRecall   : 0.0484\n\nConfusion Matrix:\n[[9149   25]\n [ 786   40]]"
  },
  {
    "objectID": "assets/6_Bayesian Network.html",
    "href": "assets/6_Bayesian Network.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pyAgrum.skbn as skbn\nimport pyAgrum.lib.notebook as gnb\n\nfrom sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, RocCurveDisplay, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, RandomizedSearchCV\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport sample_data as sd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\nfrom sklearn.inspection import permutation_importance\n\nfrom IPython.display import display,HTML\ndf = pd.read_csv(\"/Users/NathanSwan 1/Downloads/heart_cleaned_final2.csv\", nrows=50000)\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nDiabetic\n...\nRace_White\nGenHealth_Excellent\nGenHealth_Fair\nGenHealth_Good\nGenHealth_Poor\nGenHealth_Very good\nSleepCategory_Very Short\nSleepCategory_Short\nSleepCategory_Normal\nSleepCategory_Long\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n1\n...\n1\n0\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n1\n...\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n0\n...\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n5 rows × 46 columns\n\n\n\n\nimport pickle\n\n# adjust paths/filenames as needed\nwith open('/Users/NathanSwan 1/Downloads/331 Project/X_train.p', 'rb') as f:\n    X_train = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/X_test.p', 'rb') as f:\n    X_test = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/y_train.p', 'rb') as f:\n    y_train = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/y_test.p', 'rb') as f:\n    y_test = pickle.load(f)\n\n# quick check\nprint(X_train.shape, X_test.shape, len(y_train), len(y_test))\n\n(35000, 21) (15000, 21) 35000 15000\n\n\nTree Augemented Network Bayesian Network\n\n\n\nbnc = skbn.BNClassifier(learningMethod = 'TAN') ## Tree-Augmented Network.\n\nbnc.fit(X_train, y_train)\n\ny_train_hat = bnc.predict(X_train)\ny_test_hat = bnc.predict(X_test)\n\nbnc.score(X_train, y_train), bnc.score(X_test, y_test)\n\n/opt/anaconda3/envs/MISY331__base/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/MISY331__base/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n/opt/anaconda3/envs/MISY331__base/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n  warnings.warn(\n\n\n(0.7485142857142857, 0.7406)\n\n\n\ngnb.showBN(bnc.bn)\n\n\n\n\n\n\n\n\n\nwith open('bayesian_network_model.pkl', 'wb') as file:\n    pickle.dump(bnc.bn, file)"
  },
  {
    "objectID": "assets/2_Pre-Analysis Visualization.html",
    "href": "assets/2_Pre-Analysis Visualization.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"/Users/NathanSwan 1/Downloads/heart_cleaned_final2.csv\", nrows=50000)\ndf.info()\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 46 columns):\n #   Column                               Non-Null Count  Dtype  \n---  ------                               --------------  -----  \n 0   HeartDisease                         50000 non-null  int64  \n 1   BMI                                  50000 non-null  float64\n 2   Smoking                              50000 non-null  int64  \n 3   AlcoholDrinking                      50000 non-null  int64  \n 4   Stroke                               50000 non-null  int64  \n 5   PhysicalHealth                       50000 non-null  float64\n 6   MentalHealth                         50000 non-null  float64\n 7   DiffWalking                          50000 non-null  int64  \n 8   Sex                                  50000 non-null  int64  \n 9   Diabetic                             50000 non-null  int64  \n 10  PhysicalActivity                     50000 non-null  int64  \n 11  SleepTime                            50000 non-null  float64\n 12  Asthma                               50000 non-null  int64  \n 13  KidneyDisease                        50000 non-null  int64  \n 14  SkinCancer                           50000 non-null  int64  \n 15  ComorbidityCount                     50000 non-null  int64  \n 16  UnhealthyDays                        50000 non-null  float64\n 17  RiskBehavior                         50000 non-null  int64  \n 18  Age_18-24                            50000 non-null  int64  \n 19  Age_25-29                            50000 non-null  int64  \n 20  Age_30-34                            50000 non-null  int64  \n 21  Age_35-39                            50000 non-null  int64  \n 22  Age_40-44                            50000 non-null  int64  \n 23  Age_45-49                            50000 non-null  int64  \n 24  Age_50-54                            50000 non-null  int64  \n 25  Age_55-59                            50000 non-null  int64  \n 26  Age_60-64                            50000 non-null  int64  \n 27  Age_65-69                            50000 non-null  int64  \n 28  Age_70-74                            50000 non-null  int64  \n 29  Age_75-79                            50000 non-null  int64  \n 30  Age_80 or older                      50000 non-null  int64  \n 31  Race_American Indian/Alaskan Native  50000 non-null  int64  \n 32  Race_Asian                           50000 non-null  int64  \n 33  Race_Black                           50000 non-null  int64  \n 34  Race_Hispanic                        50000 non-null  int64  \n 35  Race_Other                           50000 non-null  int64  \n 36  Race_White                           50000 non-null  int64  \n 37  GenHealth_Excellent                  50000 non-null  int64  \n 38  GenHealth_Fair                       50000 non-null  int64  \n 39  GenHealth_Good                       50000 non-null  int64  \n 40  GenHealth_Poor                       50000 non-null  int64  \n 41  GenHealth_Very good                  50000 non-null  int64  \n 42  SleepCategory_Very Short             50000 non-null  int64  \n 43  SleepCategory_Short                  50000 non-null  int64  \n 44  SleepCategory_Normal                 50000 non-null  int64  \n 45  SleepCategory_Long                   50000 non-null  int64  \ndtypes: float64(5), int64(41)\nmemory usage: 17.5 MB\n\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nDiabetic\n...\nRace_White\nGenHealth_Excellent\nGenHealth_Fair\nGenHealth_Good\nGenHealth_Poor\nGenHealth_Very good\nSleepCategory_Very Short\nSleepCategory_Short\nSleepCategory_Normal\nSleepCategory_Long\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n1\n...\n1\n0\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n1\n...\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n0\n...\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n5 rows × 46 columns\n\n\n\n\nDistribution of the Dependent Variable. 0 meaning they do not have heart disease and 1 meaning they do have heart disease\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nhd_counts = df['HeartDisease'].value_counts().sort_index()\nplt.figure(figsize=(6,4))\nplt.bar(hd_counts.index, hd_counts.values)\nplt.xticks([0,1])\nplt.xlabel('Heart Disease Presence')\nplt.ylabel('Count')\nplt.title('Distribution of Heart Disease')\nplt.show()\n\n\n\n\n\n\n\n\n\nSmoking Status. 0 meaning that the patient does not smoke and 1 meaning that they do smoke.\n\n\nsmoke_counts = df['Smoking'].value_counts()\nplt.figure(figsize=(6,4))\nplt.bar(smoke_counts.index, smoke_counts.values)\nplt.xticks([0,1])\nplt.xlabel('Smoking Status')\nplt.ylabel('Count')\nplt.title('Distribution of Smoking Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nBMI Distribution By Heart Disease Status\n\n\nplt.figure(figsize=(8,5))\nsns.histplot(data=df, x=\"BMI\", hue=\"HeartDisease\", kde=True, bins=40, palette=\"coolwarm\", element=\"step\")\nplt.title(\"BMI Distribution by Heart Disease Status\")\nplt.xlabel(\"BMI\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAlcohol Drinking Status. 0 means they do not drink and 1 means they do drink.\n\n\nalc_counts = df['AlcoholDrinking'].value_counts()\nplt.figure(figsize=(6,4))\nplt.bar(alc_counts.index, alc_counts.values)\nplt.xticks([0,1])\nplt.xlabel('Alcohol Drinking Status')\nplt.ylabel('Count')\nplt.title('Distribution of Alcohol Drinking')\nplt.show()"
  },
  {
    "objectID": "assets/7_Clustering.html",
    "href": "assets/7_Clustering.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\n\n# 1. Load cleaned dataset\ndf = pd.read_csv(r\"C:\\Users\\Michael Antonucci\\OneDrive\\Documents\\MISY331 Final Project\\heart_cleaned_final.csv\")\n\n\n# 2. Encode target and categorical features\ndf[\"HeartDisease\"] = df[\"HeartDisease\"].map({\"Yes\": 1, \"No\": 0})\ndf_encoded = pd.get_dummies(df, drop_first=True)\n\n# 3. Drop target variable for unsupervised clustering\nX = df_encoded.drop(\"HeartDisease\", axis=1)\n\n\n# 4. Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 5. Elbow Method and Silhouette Scores\ninertia = []\nsil_scores = []\nK_range = range(2, 15)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertia.append(kmeans.inertia_)\n    sil_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\n\n# 6. Plot Elbow Method\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, inertia, marker='o')\nplt.title('Elbow Method: Optimal k')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.grid(True)\nplt.show()\n\n# 7. Plot Silhouette Scores\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, sil_scores, marker='o', color='green')\nplt.title('Silhouette Score for Each k')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 8. Apply KMeans with optimal k\noptimal_k = 4  # Set based on Elbow/Silhouette plot\nkmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ncluster_labels = kmeans_final.fit_predict(X_scaled)\ndf_encoded['Cluster'] = cluster_labels\n\n\n# 9. Export model and clustered data\nwith open(\"clustering_model.p\", \"wb\") as f:\n    pickle.dump(kmeans_final, f)\n\ndf_encoded.to_csv(\"heart_2020_clustered.csv\", index=False)"
  },
  {
    "objectID": "assets/9_ResultVisualization.html",
    "href": "assets/9_ResultVisualization.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "Result Visualization\n\nimport pyAgrum.skbn as skbn\nimport pyAgrum.lib.notebook as gnb\n\nfrom sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, RocCurveDisplay, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, RandomizedSearchCV\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport sample_data as sd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\nfrom sklearn.inspection import permutation_importance\n\nfrom IPython.display import display,HTML\ndf = pd.read_csv(\"/Users/NathanSwan 1/Downloads/heart_cleaned_final2.csv\", nrows=50000)\n\ndf.head()\nimport pickle\n\n# adjust paths/filenames as needed\nwith open('/Users/NathanSwan 1/Downloads/331 Project/X_train.p', 'rb') as f:\n    X_train = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/X_test.p', 'rb') as f:\n    X_test = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/y_train.p', 'rb') as f:\n    y_train = pickle.load(f)\n\nwith open('/Users/NathanSwan 1/Downloads/331 Project/y_test.p', 'rb') as f:\n    y_test = pickle.load(f)\n\n# quick check\nprint(X_train.shape, X_test.shape, len(y_train), len(y_test))\n\n(35000, 21) (15000, 21) 35000 15000\n\n\n\nRidge Regression\n\n\nimport pickle\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport pickle\nridge_model_filename = \"ridge_model.pkl\"\nwith open(ridge_model_filename, 'rb') as file:\n    loaded_ridge_model = pickle.load(file)\nprint(\"\\nLoaded Ridge Model:\")\nprint(loaded_ridge_model)\n\n\n\nLoaded Ridge Model:\nRidge(alpha=7993.511705685619)\n\n\n\nRandom Forests\n\n\n\nimport pickle\n\n# Define the filename\nfeature_importance_filename = \"rf_feature_importances.p\"\n\n# Load the feature importances\nwith open(feature_importance_filename, \"rb\") as file:\n    loaded_importances = pickle.load(file)\n\nprint(\"Feature importances loaded successfully.\\n\")\n\nfeature_names = X_train.columns  # Adjust based on your dataset\n\n# Display with feature names\nfor name, importance in zip(feature_names, loaded_importances):\n    print(f\"{name}: {importance:.4f}\")\n\nFeature importances loaded successfully.\n\nBMI: 0.2338\nSmoking: 0.0148\nAlcoholDrinking: 0.0097\nStroke: 0.0263\nPhysicalHealth: 0.0569\nMentalHealth: 0.0485\nDiffWalking: 0.0261\nSex: 0.0308\nAgeCategory: 0.0228\nRace: 0.0283\nDiabetic: 0.0626\nPhysicalActivity: 0.0147\nGenHealth: 0.0134\nSleepTime: 0.0146\nAsthma: 0.0426\nKidneyDisease: 0.0589\nSkinCancer: 0.0141\nComorbidityCount: 0.0014\nUnhealthyDays: 0.0019\nRiskBehavior: 0.0024\nSleepCategory: 0.0031\n\n\n\n\n\nBMI: 0.2338\nSmoking: 0.0148\nAlcoholDrinking: 0.0097\nStroke: 0.0263\nPhysicalHealth: 0.0569\nMentalHealth: 0.0485\nDiffWalking: 0.0261\nSex: 0.0308\nAgeCategory: 0.0228\nRace: 0.0283\nDiabetic: 0.0626\nPhysicalActivity: 0.0147\nGenHealth: 0.0134\nSleepTime: 0.0146\nAsthma: 0.0426\nKidneyDisease: 0.0589\nSkinCancer: 0.0141\nComorbidityCount: 0.0014\nUnhealthyDays: 0.0019\nRiskBehavior: 0.0024\nSleepCategory: 0.0031\n\n\n\nBoosted Trees\n\n\nfrom google.colab import files\nuploaded = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving xgb_model2.pkl to xgb_model2.pkl\n\n\n\nimport pickle\n\nwith open(\"xgb_model2.pkl\", \"rb\") as f:\n    xgb_model = pickle.load(f)\n\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:21:07] WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\nconfiguration generated by an older version of XGBoost, please export the model by calling\n`Booster.save_model` from that version first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n\nfor more details about differences between saving model and serializing.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\nBayesian Network\n\n\nwith open('bayesian_network_model.pkl', 'rb') as file:\n    loaded_model = pickle.load(file)\ngnb.showBN(bnc.bn)\n\n\n\n\n\n\n\n\nBayesian Network: The bayesian network centralizes around HeartDisease and its variables that affect it. Some of the important key variables are AgeCategory, ComobidityCount, GenHealth, and Sex. If you go throw the network some things such as ComorbidityCount are influenced by many different variables. These variables are GenHealth, KidneyDisease, Stroke, Diabetic, and Asthma which would all make sense when talking about ComorbidityCount. Some other important connects are RiskBehavior with Smoking and AlcoholDrinking which also do make sense. Overall the network does show that alot of different variables are important to factor when analyzing HeartDisease. This model has an accuracy train/test accuracy aroud 73-74%.\n\nClustering\n\n\nimport pickle\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the necessary libraries\nimport numpy as np\nimport pandas as pd\n# Define the model filename\nclustering_model_filename = \"clustering_model.p\"\n\n# Open the file in read-binary mode and load the model\nwith open(clustering_model_filename, \"rb\") as file:\n    kmeans_model = pickle.load(file)\n\nprint(\"Clustering model successfully loaded.\")\n\nClustering model successfully loaded.\n\n\n\n# Display the clustering model\nprint(\"\\nLoaded Clustering Model:\")\nprint(kmeans_model)\n\n\nLoaded Clustering Model:\nKMeans(n_clusters=2, n_init=10, random_state=42)\n\n\n\nNeural Networks\n\n\nfrom google.colab import files\nuploaded = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving neural_net_model.p to neural_net_model.p\n\n\n\nimport pickle\nfrom tensorflow.keras.models import model_from_json\n\n# Define the wrapper class (must be included again in Colab)\nclass KerasModelWrapper:\n    def __init__(self, model):\n        self.model_json = model.to_json()\n        self.model_weights = model.get_weights()\n\n    def load_model(self):\n        model = model_from_json(self.model_json)\n        model.set_weights(self.model_weights)\n        return model\n\n# Load the model from the uploaded file\nwith open(\"neural_net_model.p\", \"rb\") as f:\n    wrapper = pickle.load(f)\n\nnn_model = wrapper.load_model()\nnn_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\n\nnn_model.summary()\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_3 (Dense)                 │ (None, 64)             │         2,944 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 32)             │         2,080 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 5,057 (19.75 KB)\n\n\n\n Trainable params: 5,057 (19.75 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nThe Neural Network was trained to predict the likelihood of heart disease using all available features in the dataset. One of the major challenges we faced was class imbalance — only about 9% of the data had heart disease cases, which led to very high accuracy but poor detection of actual disease cases in the baseline model.\nTo address this, we used class weighting, which forces the model to “care more” about the minority class (heart disease). After applying this, performance improved significantly on key metrics like recall and F1-score for the heart disease class.\nThe model architecture included:\nAn input layer connected to two hidden layers with ReLU activation\nA final output layer with a sigmoid activation for binary classification\nAfter training for 30 epochs, the model showed strong improvement:\nBaseline model: Accuracy ~92%, but Recall (1) was only 15%\nBalanced model: Accuracy ~76%, but Recall (1) increased to 79%\nThis trade-off shows that while overall accuracy dropped, the model became much better at identifying people with heart disease, which is the real-world priority. This model emphasizes how neural networks can be tuned to solve imbalanced classification problems effectively using weighting."
  },
  {
    "objectID": "assets/8_Neural_Network.html",
    "href": "assets/8_Neural_Network.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"heart_cleaned_final.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\n...\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\nComorbidityCount\nUnhealthyDays\nRiskBehavior\nSleepCategory\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n55-59\n...\n1\nVery good\n5.0\n1\n0\n1\n3\n30.0\n1\nVery Short\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n80 or older\n...\n1\nVery good\n7.0\n0\n0\n0\n1\n0.0\n0\nNormal\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n65-69\n...\n1\nFair\n8.0\n1\n0\n0\n2\n30.0\n1\nNormal\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n75-79\n...\n0\nGood\n6.0\n0\n0\n1\n1\n0.0\n0\nShort\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n40-44\n...\n1\nVery good\n8.0\n0\n0\n0\n0\n28.0\n0\nNormal\n\n\n\n\n5 rows × 22 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\n...\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\nComorbidityCount\nUnhealthyDays\nRiskBehavior\nSleepCategory\n\n\n\n\n0\n0\n16.60\n1\n0\n0\n3.0\n30.0\n0\n0\n55-59\n...\n1\nVery good\n5.0\n1\n0\n1\n3\n30.0\n1\nVery Short\n\n\n1\n0\n20.34\n0\n0\n1\n0.0\n0.0\n0\n0\n80 or older\n...\n1\nVery good\n7.0\n0\n0\n0\n1\n0.0\n0\nNormal\n\n\n2\n0\n26.58\n1\n0\n0\n20.0\n30.0\n0\n1\n65-69\n...\n1\nFair\n8.0\n1\n0\n0\n2\n30.0\n1\nNormal\n\n\n3\n0\n24.21\n0\n0\n0\n0.0\n0.0\n0\n0\n75-79\n...\n0\nGood\n6.0\n0\n0\n1\n1\n0.0\n0\nShort\n\n\n4\n0\n23.71\n0\n0\n0\n28.0\n0.0\n1\n0\n40-44\n...\n1\nVery good\n8.0\n0\n0\n0\n0\n28.0\n0\nNormal\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n# Core data tools\nimport pandas as pd\nimport numpy as np\nimport pickle\n\n# Neural Network from TensorFlow (Keras)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Evaluation tools\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import class_weight\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf = pd.read_csv(\"heart_cleaned_final.csv\")\n\n# Load the pre-split data\nX_train = pickle.load(open(\"X_train.p\", \"rb\"))\nX_test = pickle.load(open(\"X_test.p\", \"rb\"))\ny_train = pickle.load(open(\"y_train.p\", \"rb\"))\ny_test = pickle.load(open(\"y_test.p\", \"rb\"))\n\n\n# Define the model architecture\nbaseline_model = Sequential()\nbaseline_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\nbaseline_model.add(Dense(32, activation='relu'))\nbaseline_model.add(Dense(1, activation='sigmoid'))  # Final layer: probability of heart disease\n\n# Compile the model\nbaseline_model.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy'])\n\nc:\\Users\\modyx\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\n# Fit the model\nbaseline_history = baseline_model.fit(X_train, y_train,\n                                      epochs=30,\n                                      batch_size=32,\n                                      validation_split=0.2,\n                                      verbose=1)\n\n\nEpoch 1/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9083 - loss: 0.2832 - val_accuracy: 0.9145 - val_loss: 0.2336\n\nEpoch 2/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9103 - loss: 0.2438 - val_accuracy: 0.9130 - val_loss: 0.2310\n\nEpoch 3/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9123 - loss: 0.2353 - val_accuracy: 0.9145 - val_loss: 0.2321\n\nEpoch 4/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9108 - loss: 0.2415 - val_accuracy: 0.9091 - val_loss: 0.2393\n\nEpoch 5/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9114 - loss: 0.2371 - val_accuracy: 0.9149 - val_loss: 0.2308\n\nEpoch 6/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9123 - loss: 0.2351 - val_accuracy: 0.9165 - val_loss: 0.2376\n\nEpoch 7/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9145 - loss: 0.2299 - val_accuracy: 0.9161 - val_loss: 0.2295\n\nEpoch 8/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9125 - loss: 0.2345 - val_accuracy: 0.9147 - val_loss: 0.2491\n\nEpoch 9/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9127 - loss: 0.2365 - val_accuracy: 0.9118 - val_loss: 0.2312\n\nEpoch 10/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 999us/step - accuracy: 0.9130 - loss: 0.2309 - val_accuracy: 0.9162 - val_loss: 0.2377\n\nEpoch 11/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9142 - loss: 0.2299 - val_accuracy: 0.9146 - val_loss: 0.2298\n\nEpoch 12/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 999us/step - accuracy: 0.9177 - loss: 0.2219 - val_accuracy: 0.9158 - val_loss: 0.2312\n\nEpoch 13/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9122 - loss: 0.2286 - val_accuracy: 0.9149 - val_loss: 0.2299\n\nEpoch 14/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 985us/step - accuracy: 0.9143 - loss: 0.2290 - val_accuracy: 0.9130 - val_loss: 0.2335\n\nEpoch 15/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9132 - loss: 0.2313 - val_accuracy: 0.9143 - val_loss: 0.2326\n\nEpoch 16/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9155 - loss: 0.2241 - val_accuracy: 0.9164 - val_loss: 0.2285\n\nEpoch 17/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 988us/step - accuracy: 0.9137 - loss: 0.2269 - val_accuracy: 0.9131 - val_loss: 0.2301\n\nEpoch 18/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 998us/step - accuracy: 0.9158 - loss: 0.2233 - val_accuracy: 0.9149 - val_loss: 0.2297\n\nEpoch 19/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 994us/step - accuracy: 0.9139 - loss: 0.2283 - val_accuracy: 0.9140 - val_loss: 0.2301\n\nEpoch 20/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 992us/step - accuracy: 0.9124 - loss: 0.2275 - val_accuracy: 0.9111 - val_loss: 0.2343\n\nEpoch 21/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9138 - loss: 0.2247 - val_accuracy: 0.9144 - val_loss: 0.2301\n\nEpoch 22/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 999us/step - accuracy: 0.9150 - loss: 0.2232 - val_accuracy: 0.9140 - val_loss: 0.2305\n\nEpoch 23/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 990us/step - accuracy: 0.9181 - loss: 0.2179 - val_accuracy: 0.9150 - val_loss: 0.2298\n\nEpoch 24/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 991us/step - accuracy: 0.9155 - loss: 0.2206 - val_accuracy: 0.9122 - val_loss: 0.2326\n\nEpoch 25/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 995us/step - accuracy: 0.9119 - loss: 0.2279 - val_accuracy: 0.9145 - val_loss: 0.2324\n\nEpoch 26/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9158 - loss: 0.2225 - val_accuracy: 0.9106 - val_loss: 0.2320\n\nEpoch 27/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 994us/step - accuracy: 0.9153 - loss: 0.2205 - val_accuracy: 0.9133 - val_loss: 0.2327\n\nEpoch 28/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9166 - loss: 0.2179 - val_accuracy: 0.9118 - val_loss: 0.2386\n\nEpoch 29/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9150 - loss: 0.2224 - val_accuracy: 0.9091 - val_loss: 0.2360\n\nEpoch 30/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 997us/step - accuracy: 0.9149 - loss: 0.2204 - val_accuracy: 0.9103 - val_loss: 0.2349\n\n\n\n\n\n# Predict class probabilities and convert to 0/1\ny_pred_prob = baseline_model.predict(X_test).flatten()\ny_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n# Classification report\nprint(\"📊 Classification Report (Baseline):\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion matrix\nplt.figure(figsize=(5, 4))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"No HD\", \"Yes HD\"], yticklabels=[\"No HD\", \"Yes HD\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix (Baseline NN)\")\nplt.tight_layout()\nplt.show()\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 0s 506us/step\n\n📊 Classification Report (Baseline):\n\n              precision    recall  f1-score   support\n\n\n\n           0       0.93      0.99      0.96      9174\n\n           1       0.48      0.15      0.23       826\n\n\n\n    accuracy                           0.92     10000\n\n   macro avg       0.70      0.57      0.59     10000\n\nweighted avg       0.89      0.92      0.90     10000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 4))\nplt.plot(baseline_history.history['accuracy'], label='Training Accuracy')\nplt.plot(baseline_history.history['val_accuracy'], label='Validation Accuracy')\nplt.title(\"Baseline NN Accuracy Over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nweights = class_weight.compute_class_weight(class_weight='balanced',\n                                             classes=np.unique(y_train),\n                                             y=y_train)\nclass_weights = {0: weights[0], 1: weights[1]}\nprint(\"Class Weights:\", class_weights)\n\nClass Weights: {0: 0.5483358008444371, 1: 5.672149744753262}\n\n\n\nweighted_model = Sequential()\nweighted_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\nweighted_model.add(Dense(32, activation='relu'))\nweighted_model.add(Dense(1, activation='sigmoid'))\n\nweighted_model.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy'])\n\nc:\\Users\\modyx\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\nweighted_history = weighted_model.fit(X_train, y_train,\n                                      epochs=30,\n                                      batch_size=32,\n                                      validation_split=0.2,\n                                      class_weight=class_weights,\n                                      verbose=1)\n\n\nEpoch 1/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.7152 - loss: 0.5922 - val_accuracy: 0.7216 - val_loss: 0.5484\n\nEpoch 2/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7394 - loss: 0.5171 - val_accuracy: 0.8593 - val_loss: 0.3162\n\nEpoch 3/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7399 - loss: 0.5154 - val_accuracy: 0.7150 - val_loss: 0.5535\n\nEpoch 4/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7327 - loss: 0.5103 - val_accuracy: 0.7136 - val_loss: 0.5528\n\nEpoch 5/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7458 - loss: 0.4904 - val_accuracy: 0.6879 - val_loss: 0.5949\n\nEpoch 6/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7362 - loss: 0.5057 - val_accuracy: 0.6770 - val_loss: 0.6056\n\nEpoch 7/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7352 - loss: 0.5008 - val_accuracy: 0.6840 - val_loss: 0.5854\n\nEpoch 8/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7424 - loss: 0.4842 - val_accuracy: 0.6565 - val_loss: 0.6620\n\nEpoch 9/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7354 - loss: 0.4931 - val_accuracy: 0.6645 - val_loss: 0.6050\n\nEpoch 10/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7365 - loss: 0.4765 - val_accuracy: 0.6998 - val_loss: 0.5677\n\nEpoch 11/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7369 - loss: 0.4797 - val_accuracy: 0.7184 - val_loss: 0.5361\n\nEpoch 12/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7350 - loss: 0.4815 - val_accuracy: 0.6965 - val_loss: 0.5610\n\nEpoch 13/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7355 - loss: 0.4837 - val_accuracy: 0.7511 - val_loss: 0.4821\n\nEpoch 14/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7447 - loss: 0.4747 - val_accuracy: 0.7423 - val_loss: 0.4979\n\nEpoch 15/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7289 - loss: 0.4841 - val_accuracy: 0.7271 - val_loss: 0.5178\n\nEpoch 16/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7312 - loss: 0.4911 - val_accuracy: 0.7886 - val_loss: 0.4270\n\nEpoch 17/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7502 - loss: 0.4717 - val_accuracy: 0.7341 - val_loss: 0.4948\n\nEpoch 18/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7386 - loss: 0.4792 - val_accuracy: 0.7529 - val_loss: 0.4799\n\nEpoch 19/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7429 - loss: 0.4737 - val_accuracy: 0.7315 - val_loss: 0.5027\n\nEpoch 20/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7347 - loss: 0.4808 - val_accuracy: 0.7659 - val_loss: 0.4455\n\nEpoch 21/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7426 - loss: 0.4778 - val_accuracy: 0.6425 - val_loss: 0.6431\n\nEpoch 22/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7346 - loss: 0.4747 - val_accuracy: 0.7401 - val_loss: 0.4866\n\nEpoch 23/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7474 - loss: 0.4722 - val_accuracy: 0.6854 - val_loss: 0.5816\n\nEpoch 24/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7410 - loss: 0.4655 - val_accuracy: 0.7502 - val_loss: 0.4979\n\nEpoch 25/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7387 - loss: 0.4751 - val_accuracy: 0.7086 - val_loss: 0.5334\n\nEpoch 26/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7386 - loss: 0.4625 - val_accuracy: 0.7045 - val_loss: 0.5512\n\nEpoch 27/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 996us/step - accuracy: 0.7435 - loss: 0.4693 - val_accuracy: 0.7349 - val_loss: 0.4896\n\nEpoch 28/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7478 - loss: 0.4556 - val_accuracy: 0.7351 - val_loss: 0.4982\n\nEpoch 29/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 993us/step - accuracy: 0.7462 - loss: 0.4669 - val_accuracy: 0.7370 - val_loss: 0.4889\n\nEpoch 30/30\n\n1000/1000 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.7461 - loss: 0.4556 - val_accuracy: 0.7456 - val_loss: 0.4749\n\n\n\n\n\n# Predict\ny_pred_prob = weighted_model.predict(X_test).flatten()\ny_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n# Report\nprint(\"📊 Classification Report (With Class Weights):\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\nplt.figure(figsize=(5, 4))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"No HD\", \"Yes HD\"], yticklabels=[\"No HD\", \"Yes HD\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix (Balanced NN)\")\nplt.tight_layout()\nplt.show()\n\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 0s 516us/step\n\n📊 Classification Report (With Class Weights):\n\n              precision    recall  f1-score   support\n\n\n\n           0       0.98      0.76      0.85      9174\n\n           1       0.23      0.79      0.35       826\n\n\n\n    accuracy                           0.76     10000\n\n   macro avg       0.60      0.77      0.60     10000\n\nweighted avg       0.91      0.76      0.81     10000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 4))\nplt.plot(weighted_history.history['accuracy'], label='Training Accuracy')\nplt.plot(weighted_history.history['val_accuracy'], label='Validation Accuracy')\nplt.title(\"NN Accuracy Over Epochs (With Class Weights)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "assets/5_BoostedTree.html",
    "href": "assets/5_BoostedTree.html",
    "title": "Owen Tatlonghari Portfolio",
    "section": "",
    "text": "Step 1: Data Cleaning\n\nimport pandas as pd\ndf = pd.read_csv(\"heart_2020_cleaned.csv\", nrows=50000)\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 18 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   HeartDisease      50000 non-null  object \n 1   BMI               50000 non-null  float64\n 2   Smoking           50000 non-null  object \n 3   AlcoholDrinking   50000 non-null  object \n 4   Stroke            50000 non-null  object \n 5   PhysicalHealth    50000 non-null  float64\n 6   MentalHealth      50000 non-null  float64\n 7   DiffWalking       50000 non-null  object \n 8   Sex               50000 non-null  object \n 9   AgeCategory       50000 non-null  object \n 10  Race              50000 non-null  object \n 11  Diabetic          50000 non-null  object \n 12  PhysicalActivity  50000 non-null  object \n 13  GenHealth         50000 non-null  object \n 14  SleepTime         50000 non-null  float64\n 15  Asthma            50000 non-null  object \n 16  KidneyDisease     50000 non-null  object \n 17  SkinCancer        50000 non-null  object \ndtypes: float64(4), object(14)\nmemory usage: 6.9+ MB\n\n\n\n\n\n\n\n\n\nHeartDisease\nBMI\nSmoking\nAlcoholDrinking\nStroke\nPhysicalHealth\nMentalHealth\nDiffWalking\nSex\nAgeCategory\nRace\nDiabetic\nPhysicalActivity\nGenHealth\nSleepTime\nAsthma\nKidneyDisease\nSkinCancer\n\n\n\n\n0\nNo\n16.60\nYes\nNo\nNo\n3.0\n30.0\nNo\nFemale\n55-59\nWhite\nYes\nYes\nVery good\n5.0\nYes\nNo\nYes\n\n\n1\nNo\n20.34\nNo\nNo\nYes\n0.0\n0.0\nNo\nFemale\n80 or older\nWhite\nNo\nYes\nVery good\n7.0\nNo\nNo\nNo\n\n\n2\nNo\n26.58\nYes\nNo\nNo\n20.0\n30.0\nNo\nMale\n65-69\nWhite\nYes\nYes\nFair\n8.0\nYes\nNo\nNo\n\n\n3\nNo\n24.21\nNo\nNo\nNo\n0.0\n0.0\nNo\nFemale\n75-79\nWhite\nNo\nNo\nGood\n6.0\nNo\nNo\nYes\n\n\n4\nNo\n23.71\nNo\nNo\nNo\n28.0\n0.0\nYes\nFemale\n40-44\nWhite\nNo\nYes\nVery good\n8.0\nNo\nNo\nNo\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n# Plotting the number of people with and without heart disease\nplt.figure(figsize=(8, 6))\ndf['HeartDisease'].value_counts().plot(kind='bar')\nplt.title('Number of People With and Without Heart Disease (First 50,000 Rows)')\nplt.xlabel('Heart Disease')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nbinary_cols = [\n    'HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke',\n    'DiffWalking', 'Diabetic', 'PhysicalActivity',\n    'Asthma', 'KidneyDisease', 'SkinCancer'\n]\n\ndf[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n/var/folders/gp/9b_fhx0s24s2954c8mqgvr240000gn/T/ipykernel_60282/918804490.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == 'Yes' else 0)\n\n\n\ndf['Sex'] = df['Sex'].map({'Male': 1, 'Female': 0})\n\n\ndf['ComorbidityCount'] = df[['Stroke', 'Diabetic', 'Asthma', 'KidneyDisease', 'SkinCancer']].sum(axis=1)\n\n\ndf['UnhealthyDays'] = df['PhysicalHealth'] + df['MentalHealth']\ndf['UnhealthyDays'] = df['UnhealthyDays'].clip(upper=30)\n\n\ndf['RiskBehavior'] = ((df['Smoking'] == 1) | (df['AlcoholDrinking'] == 1)).astype(int)\n\n\ndf['SleepCategory'] = pd.cut(df['SleepTime'],\n                             bins=[0, 5, 6.9, 8.9, 24],\n                             labels=['Very Short', 'Short', 'Normal', 'Long'])\n\n\ndf = pd.get_dummies(df, columns = ['AgeCategory', 'Race', 'GenHealth', 'SleepCategory'], \n                    prefix = ['Age', 'Race', 'GenHealth', 'SleepCategory'], drop_first=False, dtype = int)\n\n\ndf.dtypes\n\nHeartDisease                             int64\nBMI                                    float64\nSmoking                                  int64\nAlcoholDrinking                          int64\nStroke                                   int64\nPhysicalHealth                         float64\nMentalHealth                           float64\nDiffWalking                              int64\nSex                                      int64\nDiabetic                                 int64\nPhysicalActivity                         int64\nSleepTime                              float64\nAsthma                                   int64\nKidneyDisease                            int64\nSkinCancer                               int64\nComorbidityCount                         int64\nUnhealthyDays                          float64\nRiskBehavior                             int64\nAge_18-24                                int64\nAge_25-29                                int64\nAge_30-34                                int64\nAge_35-39                                int64\nAge_40-44                                int64\nAge_45-49                                int64\nAge_50-54                                int64\nAge_55-59                                int64\nAge_60-64                                int64\nAge_65-69                                int64\nAge_70-74                                int64\nAge_75-79                                int64\nAge_80 or older                          int64\nRace_American Indian/Alaskan Native      int64\nRace_Asian                               int64\nRace_Black                               int64\nRace_Hispanic                            int64\nRace_Other                               int64\nRace_White                               int64\nGenHealth_Excellent                      int64\nGenHealth_Fair                           int64\nGenHealth_Good                           int64\nGenHealth_Poor                           int64\nGenHealth_Very good                      int64\nSleepCategory_Very Short                 int64\nSleepCategory_Short                      int64\nSleepCategory_Normal                     int64\nSleepCategory_Long                       int64\ndtype: object\n\n\n\ndf.to_csv(\"heart_cleaned_final.csv\", index=False)\n\n\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n\nX = df.drop(\"HeartDisease\", axis=1)  \ny = df[\"HeartDisease\"]              \n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Step 3: Save each set as a Pickle (.p) file\npickle.dump(X_train, open(\"X_train.p\", \"wb\"))\npickle.dump(X_test, open(\"X_test.p\", \"wb\"))\npickle.dump(y_train, open(\"y_train.p\", \"wb\"))\npickle.dump(y_test, open(\"y_test.p\", \"wb\"))\n\nprint(\"Pickle files saved successfully!\")\n\nPickle files saved successfully!\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV, RandomizedSearchCV\n\nfrom sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate\nimport numpy as np\nimport pandas as pd\nimport sample_data as sd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\nrs = 1234\n\n\nspw = (y_train.shape[0] - np.sum(y_train)) / np.sum(y_train)\nspw\n\n10.490479317137229\n\n\n\ndf.dtypes\n\nHeartDisease                             int64\nBMI                                    float64\nSmoking                                  int64\nAlcoholDrinking                          int64\nStroke                                   int64\nPhysicalHealth                         float64\nMentalHealth                           float64\nDiffWalking                              int64\nSex                                      int64\nDiabetic                                 int64\nPhysicalActivity                         int64\nSleepTime                              float64\nAsthma                                   int64\nKidneyDisease                            int64\nSkinCancer                               int64\nComorbidityCount                         int64\nUnhealthyDays                          float64\nRiskBehavior                             int64\nAge_18-24                                int64\nAge_25-29                                int64\nAge_30-34                                int64\nAge_35-39                                int64\nAge_40-44                                int64\nAge_45-49                                int64\nAge_50-54                                int64\nAge_55-59                                int64\nAge_60-64                                int64\nAge_65-69                                int64\nAge_70-74                                int64\nAge_75-79                                int64\nAge_80 or older                          int64\nRace_American Indian/Alaskan Native      int64\nRace_Asian                               int64\nRace_Black                               int64\nRace_Hispanic                            int64\nRace_Other                               int64\nRace_White                               int64\nGenHealth_Excellent                      int64\nGenHealth_Fair                           int64\nGenHealth_Good                           int64\nGenHealth_Poor                           int64\nGenHealth_Very good                      int64\nSleepCategory_Very Short                 int64\nSleepCategory_Short                      int64\nSleepCategory_Normal                     int64\nSleepCategory_Long                       int64\ndtype: object\n\n\n\nfrom xgboost import XGBClassifier ## There's more variety here as well XGBRegressor, XGBFClassifier, XGBFRegressor\n\nxg_cl = XGBClassifier(objective='binary:logistic', ## Objective function can be changed for specific case - there are many options!\n                        n_estimators = 100, ## Default is 100\n                        max_depth = 6, ## Default is 6\n                        eta = 0.3, ## AKA learning_rate, but XGB restricts it to [0,1], Default = 0.3,\n                        scale_pos_weight = spw, ## Treat the datata as balanced\n                        seed = rs) ## different name, same thing, random_state\n\nxg_cl.fit(X_train,y_train)\n\n#Predict the model - works the same\ny_train_hat = xg_cl.predict(X_train)\ny_test_hat = xg_cl.predict(X_test)\n\n## We do still have the 'score' function as in sklearn\nxg_cl.score(X_train, y_train), xg_cl.score(X_test, y_test) ## we set `binary:logisitc` on a Classifier, so score is Accuracy\n\n(0.8542857142857143, 0.7976666666666666)\n\n\n\n## Base\nclf = XGBClassifier(objective='binary:logistic', scale_pos_weight = spw, seed = rs) \n\n## KFold Stratified\nfrom sklearn.model_selection import StratifiedKFold\ncv_5 = StratifiedKFold(n_splits=5)\n\n## Parameter Grid\nparam_grid = {'max_depth': np.arange(5,12,1),\n              'n_estimators': np.arange(75,100,5),\n              'eta': np.arange(.3,.5,.1),\n              'min_child_weight': [1, 2, 3],\n            'scale_pos_weight': [1, 2, 3]}\n\n## Run the Grid search\nsh = RandomizedSearchCV(clf,\n                         param_grid,\n                         n_iter = 50,\n                         cv = cv_5,\n                         random_state = rs,).fit(X_train, y_train)\nsh.best_estimator_\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eta=0.4, eval_metric=None,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n              max_leaves=None, min_child_weight=1, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=75,\n              n_jobs=None, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriFittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eta=0.4, eval_metric=None,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n              max_leaves=None, min_child_weight=1, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=75,\n              n_jobs=None, num_parallel_tree=None, ...) \n\n\n\nxg_cl = sh.best_estimator_\n\n#Predict the model - works the same\ny_train_hat = xg_cl.predict(X_train)\ny_test_hat = xg_cl.predict(X_test)\n\n## We do still have the 'score' function as in sklearn\nxg_cl.score(X_train, y_train), xg_cl.score(X_test, y_test)\n\n(0.9293428571428571, 0.9118666666666667)\n\n\n\nlabels_ = ['No Disease', 'Disease']\n\ncm_train = confusion_matrix(y_train, y_train_hat, normalize='true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=labels_)\ndisp.plot()\nplt.title(\"Predicting Heart Disease by XGB Train Set\")\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\ncm_test = confusion_matrix(y_test, y_test_hat, normalize='true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels= labels_)\ndisp.plot()\nplt.title(\"Predicting Heart Disease by XGB Test Set\")\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import f1_score, roc_auc_score, roc_curve\n\n\n# Predict on test set\ny_pred = xg_cl.predict(X_test)\n\n# Evaluate\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Print results\nprint(f\"Accuracy:  {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall:    {recall:.2f}\")\n\nAccuracy:  0.91\nPrecision: 0.48\nRecall:    0.14\n\n\n\n# Predict probabilities for ROC AUC\ny_proba = xg_cl.predict_proba(X_test)[:, 1]  # Probability of class 1\n\n# Compute metrics\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nprint(f\"F1 Score:  {f1:.2f}\")\nprint(f\"ROC AUC:   {roc_auc:.2f}\")\n\nF1 Score:  0.22\nROC AUC:   0.82\n\n\nThe model has high accuracy (0.91) but low recall (0.14), meaning it misses most heart disease cases. Precision (0.48) is also low, leading to false positives. The F1 score (0.22) reflects poor balance between precision and recall, while the ROC AUC (0.82) shows good overall class differentiation. Improving recall is crucial to avoid missing heart disease cases. Probable Cause: Dataset Class Imbalance between NoHeartDisease/HeartDisease respectively,(45,648/4,352)\n\ndf_tree = xg_cl.get_booster().trees_to_dataframe()\ndf_tree.head(10)\n\n\n\n\n\n\n\n\nTree\nNode\nID\nFeature\nSplit\nYes\nNo\nMissing\nGain\nCover\nCategory\n\n\n\n\n0\n0\n0\n0-0\nComorbidityCount\n2.0\n0-1\n0-2\n0-2\n976.216797\n4724.306150\nNaN\n\n\n1\n0\n1\n0-1\nDiffWalking\n1.0\n0-3\n0-4\n0-4\n419.088623\n4347.576170\nNaN\n\n\n2\n0\n2\n0-2\nStroke\n1.0\n0-5\n0-6\n0-6\n129.842499\n376.729645\nNaN\n\n\n3\n0\n3\n0-3\nAge_80 or older\n1.0\n0-7\n0-8\n0-8\n149.806885\n3830.737300\nNaN\n\n\n4\n0\n4\n0-4\nStroke\n1.0\n0-9\n0-10\n0-10\n65.573822\n516.839050\nNaN\n\n\n5\n0\n5\n0-5\nDiffWalking\n1.0\n0-11\n0-12\n0-12\n69.850998\n268.205597\nNaN\n\n\n6\n0\n6\n0-6\nGenHealth_Very good\n1.0\n0-13\n0-14\n0-14\n33.572876\n108.524055\nNaN\n\n\n7\n0\n7\n0-7\nComorbidityCount\n1.0\n0-15\n0-16\n0-16\n105.684814\n3599.786130\nNaN\n\n\n8\n0\n8\n0-8\nSex\n1.0\n0-17\n0-18\n0-18\n35.201023\n230.951080\nNaN\n\n\n9\n0\n9\n0-9\nGenHealth_Poor\n1.0\n0-19\n0-20\n0-20\n52.831093\n489.573090\nNaN\n\n\n\n\n\n\n\n\ndf_tree.shape\n\n(3841, 11)\n\n\n\n## tells us how many splits were done \ndf_tree[df_tree['Feature'] != 'Leaf'].shape[0]\n\n1883\n\n\n\ndf_tree_features = df_tree[df_tree['Feature'] != 'Leaf'].copy()\ndf_tree_features\n\n\n\n\n\n\n\n\nTree\nNode\nID\nFeature\nSplit\nYes\nNo\nMissing\nGain\nCover\nCategory\n\n\n\n\n0\n0\n0\n0-0\nComorbidityCount\n2.000000\n0-1\n0-2\n0-2\n976.216797\n4724.306150\nNaN\n\n\n1\n0\n1\n0-1\nDiffWalking\n1.000000\n0-3\n0-4\n0-4\n419.088623\n4347.576170\nNaN\n\n\n2\n0\n2\n0-2\nStroke\n1.000000\n0-5\n0-6\n0-6\n129.842499\n376.729645\nNaN\n\n\n3\n0\n3\n0-3\nAge_80 or older\n1.000000\n0-7\n0-8\n0-8\n149.806885\n3830.737300\nNaN\n\n\n4\n0\n4\n0-4\nStroke\n1.000000\n0-9\n0-10\n0-10\n65.573822\n516.839050\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3808\n74\n24\n74-24\nBMI\n36.580002\n74-47\n74-48\n74-48\n0.732548\n4.485965\nNaN\n\n\n3809\n74\n25\n74-25\nGenHealth_Very good\n1.000000\n74-49\n74-50\n74-50\n1.045059\n4.408062\nNaN\n\n\n3810\n74\n26\n74-26\nSex\n1.000000\n74-51\n74-52\n74-52\n3.796722\n8.525488\nNaN\n\n\n3811\n74\n27\n74-27\nMentalHealth\n5.000000\n74-53\n74-54\n74-54\n3.579046\n15.346827\nNaN\n\n\n3812\n74\n28\n74-28\nSleepTime\n8.000000\n74-55\n74-56\n74-56\n1.515866\n5.987767\nNaN\n\n\n\n\n1883 rows × 11 columns\n\n\n\n\ndf_import = df_tree_features[['Feature','Gain']].copy()\n\n## We can look at summary stats using .groupby()\n## Let's look at mean, max, and quantile(0.95)\ndf_import.groupby('Feature')[['Gain']].quantile(0.95).sort_values(by = 'Gain',ascending=False).reset_index()\n\n\n\n\n\n\n\n\nFeature\nGain\n\n\n\n\n0\nDiffWalking\n115.867584\n\n\n1\nAge_80 or older\n91.714090\n\n\n2\nGenHealth_Excellent\n91.030835\n\n\n3\nStroke\n71.457689\n\n\n4\nSmoking\n49.810649\n\n\n5\nAge_25-29\n44.986475\n\n\n6\nGenHealth_Good\n44.838613\n\n\n7\nAge_18-24\n40.519531\n\n\n8\nAge_35-39\n40.496554\n\n\n9\nAge_75-79\n40.467848\n\n\n10\nAge_30-34\n38.670660\n\n\n11\nSex\n36.303448\n\n\n12\nGenHealth_Poor\n35.768389\n\n\n13\nAge_40-44\n35.579798\n\n\n14\nGenHealth_Very good\n33.748779\n\n\n15\nAsthma\n31.112913\n\n\n16\nComorbidityCount\n26.316109\n\n\n17\nAge_45-49\n25.740568\n\n\n18\nAge_70-74\n25.322577\n\n\n19\nRace_White\n18.587953\n\n\n20\nAge_65-69\n14.936148\n\n\n21\nAge_50-54\n12.706296\n\n\n22\nDiabetic\n11.539001\n\n\n23\nRace_Other\n11.461710\n\n\n24\nKidneyDisease\n9.889219\n\n\n25\nSkinCancer\n8.241578\n\n\n26\nSleepTime\n7.526331\n\n\n27\nAge_60-64\n7.401216\n\n\n28\nGenHealth_Fair\n7.102598\n\n\n29\nSleepCategory_Short\n7.066956\n\n\n30\nPhysicalHealth\n6.997932\n\n\n31\nBMI\n6.637537\n\n\n32\nUnhealthyDays\n6.189965\n\n\n33\nPhysicalActivity\n5.800232\n\n\n34\nMentalHealth\n5.638998\n\n\n35\nAge_55-59\n5.616385\n\n\n36\nRace_Hispanic\n5.203843\n\n\n37\nRace_Black\n5.084721\n\n\n38\nSleepCategory_Normal\n4.828359\n\n\n39\nAlcoholDrinking\n4.588219\n\n\n40\nRiskBehavior\n4.120059\n\n\n41\nRace_American Indian/Alaskan Native\n3.736672\n\n\n42\nRace_Asian\n2.006217\n\n\n\n\n\n\n\n\norder_ = df_import.groupby('Feature')['Gain'].mean().sort_values(ascending=False).nlargest(20).index\n\nsns.barplot(data = df_import,\n            y = 'Feature',\n            x = 'Gain', \n            errorbar = 'sd', \n            order = order_);\nplt.title(\"Feature Gain of XGB\");\n\n\n\n\n\n\n\n\n\ndf_tree_features.groupby(['Feature'])[['Split']].count().reset_index().sort_values(\"Split\", ascending = False)\n\n\n\n\n\n\n\n\nFeature\nSplit\n\n\n\n\n15\nBMI\n463\n\n\n39\nSleepTime\n142\n\n\n27\nPhysicalHealth\n117\n\n\n42\nUnhealthyDays\n111\n\n\n25\nMentalHealth\n106\n\n\n16\nComorbidityCount\n88\n\n\n35\nSex\n58\n\n\n23\nGenHealth_Very good\n41\n\n\n26\nPhysicalActivity\n38\n\n\n41\nStroke\n37\n\n\n17\nDiabetic\n37\n\n\n40\nSmoking\n34\n\n\n14\nAsthma\n33\n\n\n18\nDiffWalking\n33\n\n\n33\nRace_White\n32\n\n\n12\nAge_80 or older\n31\n\n\n38\nSleepCategory_Short\n29\n\n\n20\nGenHealth_Fair\n28\n\n\n19\nGenHealth_Excellent\n27\n\n\n31\nRace_Hispanic\n25\n\n\n21\nGenHealth_Good\n25\n\n\n9\nAge_65-69\n25\n\n\n24\nKidneyDisease\n24\n\n\n30\nRace_Black\n24\n\n\n36\nSkinCancer\n21\n\n\n37\nSleepCategory_Normal\n21\n\n\n8\nAge_60-64\n21\n\n\n11\nAge_75-79\n20\n\n\n10\nAge_70-74\n20\n\n\n22\nGenHealth_Poor\n18\n\n\n0\nAge_18-24\n15\n\n\n5\nAge_45-49\n15\n\n\n6\nAge_50-54\n14\n\n\n13\nAlcoholDrinking\n13\n\n\n7\nAge_55-59\n13\n\n\n34\nRiskBehavior\n13\n\n\n1\nAge_25-29\n12\n\n\n28\nRace_American Indian/Alaskan Native\n12\n\n\n2\nAge_30-34\n11\n\n\n32\nRace_Other\n11\n\n\n4\nAge_40-44\n10\n\n\n3\nAge_35-39\n10\n\n\n29\nRace_Asian\n5\n\n\n\n\n\n\n\n\n## The Histplot is a lot easier now actually from the df_features table\n\n## I still need to force an ordering on the Variable Column to \n## organize the HistPlot\n## I will resuse the `order_` from before\ndf_import['Feauture'] = pd.Categorical(df_import['Feature'],\n                                       ordered = True,\n                                       categories = order_)\n\nsns.histplot(data = df_import[df_import['Gain'] &lt; 1000],\n            y = 'Feature',\n            x = 'Gain');\nplt.title(\"Feature Gain of XGB\");\n\n\n\n\n\n\n\n\n\nwith open('xgb_model2.pkl', 'wb') as f:\n    pickle.dump(xg_cl, f)"
  }
]